{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the results\n",
    "\n",
    "> Ofter overlooked, preparing a good validation pipeline is crucial to getting a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastcore.basics import patch\n",
    "from fastcore.foundation import L\n",
    "\n",
    "from sleepstagingidal.data import *\n",
    "from sleepstagingidal.dataa import *\n",
    "from sleepstagingidal.dataa import swap_dict\n",
    "from sleepstagingidal.feature_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import yasa\n",
    "from rich.progress import track\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path_data = \"/media/2tbraid/antonia/PSG/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = glob(os.path.join(path_data, \"*.edf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient-Fold\n",
    "\n",
    "Before trying a lot of different configurations for the models or different feature extraction techniques, it's crucial to set up a truthful way of knowing how are this changes affecting our results. Because of that, we're going to lay out the fundation of our validation pipeline: the Patient-Fold.\n",
    "\n",
    "By similarity with traditional K-Fold, we are going to separate all the recordings we have and, iteratively, train with some of them while testing with a different set. This way of performing cross-validation will give us a good estimate on the inter-patient generalization capability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PatientFold():\n",
    "    \"\"\"Manager to perform the so-called PatientFold.\"\"\"\n",
    "    def __init__(self,\n",
    "                 path_files: List[str], # Path to the `.edf` files we want to use.\n",
    "                 n_splits: int, # Number of folds to use.\n",
    "                 random_state: int, # Random seed for reproducibility\n",
    "                 ): \n",
    "        self.path_files = path_files\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.folds = KFold(len(path_files))\n",
    "        self._patients = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing the raw `.edf` files takes quite a lot of time, so it can be very convenient to separate that part from the cross-validation part. Keep in mind that we can do this without collapsin the memory from the server because the loaded files themselves load the data in a lazy way. The best way to ensure that the loading and preprocessing is done only once is to use a `property`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def patients(self: PatientFold):\n",
    "    \"\"\"Ensures that the `.edf` files are only loaded and preprocessed once.\"\"\"\n",
    "    if self._patients is None:\n",
    "        self._patients = L([read_clean_edf(path, resample=100, bandpass=(0.3, 49)) for path in track(self.path_files, description=\"Pre-processing recordings\")])\n",
    "    return self._patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that different recordings may have different encodings for the same sleep stage, so we should be unifying them before joining data from different recordings. The easiest way to do it is turning them into their human-readable representation, and encode all of them together to ensure that all of them are encoded in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti feature_extraction\n",
    "\n",
    "def unify_labels(events: List[np.array], # List of events corresponding to different recordings encoded.\n",
    "                 mappings: List[Dict], # List of mappings to turn the encoded labels into human-readable labels.\n",
    "                 ) -> List[List[str]]: # List of labels arrays corresponding to different recordings in human-redable form.\n",
    "    return [map_events(events_, swap_dict(mapping)) for events_, mapping in zip(events, mappings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti feature_extraction\n",
    "\n",
    "def unify_labels_from_epochs(epochs: List[mne.epochs.Epochs], # List of `mne.epochs.Epochs`.\n",
    "                             ) -> List[List[str]]: # List of labels arrays corresponding to different recordings in human-redable form.\n",
    "    events = [e.events for e in epochs]\n",
    "    mappings = [e.event_id for e in epochs]\n",
    "    return [map_events(events_, swap_dict(mapping)) for events_, mapping in zip(events, mappings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can build a simple function to build the appropriate input data and its labels from a set of patients loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti feature_extraction\n",
    "\n",
    "def get_trainable_from_patients(patients: List[mne.io.edf.edf.RawEDF], # List of loaded Raw `.edf` files.\n",
    "                                channels: List[str], # Channels to be used.\n",
    "                                feature_extraction_fn, # Function to be applied to the `Epochs` to extract features.\n",
    "                                ) -> Tuple[np.array, np.array]: # X and Y data ready to be used to train a model.\n",
    "    \"\"\"\n",
    "    Extract epochs and features from `patients` and concatenate all of them \n",
    "    so that the output can be used to directly train a model.\n",
    "    \"\"\"\n",
    "    features_all, labels_all = [], []\n",
    "    for patient in track(patients, description=\"Building data from recordings...\"):\n",
    "        epochs, sr = get_epochs(patient, channels=channels)\n",
    "        features = feature_extraction_fn(epochs)\n",
    "        labels = map_events(epochs.events, swap_dict(epochs.event_id))\n",
    "        features_all.append(features)\n",
    "        labels_all.append(labels)\n",
    "    features_all, labels_all = np.concatenate(features_all), np.concatenate(labels_all)\n",
    "    return features_all, labels_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the process to be as streamlined as possible, so we can implement a `.fit()` method to quickly perform the Patient-Fold with any estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@patch\n",
    "def fit(self: PatientFold,\n",
    "        estimator, # Any object implementing a `.fit()` method to be crossvalidated. Must not be instantiated.\n",
    "        **kwargs, # Key-word arguments to be passed to the estimator at instance time.\n",
    "        ): # Results from the Patiend-Fold.\n",
    "    \"\"\"\n",
    "    Performs the cross-validation loop by training the `estimator` on the different folds\n",
    "    and returns the results.\n",
    "    \"\"\"\n",
    "    results = {\"train\":[], \"test\":[], \"model\":[]}\n",
    "    for train_idx, test_idx in self.folds.split(self.patients):\n",
    "        ## Separate according to the indexes\n",
    "        train_patients = self.patients[train_idx]\n",
    "        test_patients = self.patients[test_idx]\n",
    "        \n",
    "        ## Build data\n",
    "        X_train, Y_train = get_trainable_from_patients(train_patients, channels=channels, feature_extraction_fn=calculate_bandpower)\n",
    "        X_test, Y_test = get_trainable_from_patients(test_patients, channels=channels, feature_extraction_fn=calculate_bandpower)\n",
    "\n",
    "        ## Encode labels\n",
    "        le = LabelEncoder()\n",
    "        le.fit(Y_train)\n",
    "        Y_train, Y_test = le.transform(Y_train), le.transform(Y_test)\n",
    "        \n",
    "        ## Train the model\n",
    "        model = estimator(**kwargs, random_state=self.random_state)\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        ## Obtain the metrics of interest\n",
    "        results[\"train\"].append(model.score(X_train, Y_train))\n",
    "        results[\"test\"].append(model.score(X_test, Y_test))\n",
    "        results[\"model\"].append(model)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PatientFold(path_files=path_files[:2],\n",
    "                 n_splits=len(path_files[:2]),\n",
    "                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc1cb2fb3744270aec8c4b3a066e620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c66820f2408498d8ec01b4c5c7c3b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using data from preloaded Raw for 765 events and 3000 original time points ...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using data from preloaded Raw for 765 events and 3000 original time points ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1 bad epochs dropped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1 bad epochs dropped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d55c2e10289432cb1e30d194bb1ef28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using data from preloaded Raw for 719 events and 3000 original time points ...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using data from preloaded Raw for 719 events and 3000 original time points ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1 bad epochs dropped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1 bad epochs dropped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248ae0862cf149afb11dc794d7c2468c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using data from preloaded Raw for 719 events and 3000 original time points ...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using data from preloaded Raw for 719 events and 3000 original time points ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1 bad epochs dropped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1 bad epochs dropped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa2917f384741b7b2765f750d3b50d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using data from preloaded Raw for 765 events and 3000 original time points ...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using data from preloaded Raw for 765 events and 3000 original time points ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">1 bad epochs dropped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "1 bad epochs dropped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Sleep stage N3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/utils/_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/utils/_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnan_value\n\u001b[0;32m--> 158\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sleep stage N3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [20], line 25\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, estimator, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m     24\u001b[0m le\u001b[38;5;241m.\u001b[39mfit(Y_train)\n\u001b[0;32m---> 25\u001b[0m Y_train, Y_test \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(Y_train), \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m## Train the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m estimator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:138\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m--> 138\u001b[0m \u001b[39mreturn\u001b[39;00m _encode(y, uniques\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_)\n",
      "File \u001b[0;32m~/miniconda3/envs/sleepstaging/lib/python3.8/site-packages/sklearn/utils/_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'Sleep stage N3'"
     ]
    }
   ],
   "source": [
    "pf.fit(RandomForestClassifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('sleepstaging')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
