{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "> Extracting meaningful features from the data is always important, but plays a key role when our data is of very high dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path_data = \"/media/2tbraid/antonia/PSG/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "from multiprocessing.spawn import import_main_path\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "from rich.progress import track\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import yasa\n",
    "\n",
    "from sleepstagingidal.data import *\n",
    "from sleepstagingidal.dataa import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amplitude-independent features\n",
    "\n",
    "In our case, we have downsampled our data and have 3000 data points per epoch (30s fragment of the recording), but this is a huge amount of features to be fed into a model. Getting even further, we have the problem of calibration and normalization, which is aggravated even more when working with medical data (each hospital may work slightly different). Because of this, we are going to employ features that are independent of the amplitude of the signal: we are going to utilize mainly frequency-related features.\n",
    "\n",
    "We can use the library `yasa` to perform a basic feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = glob(os.path.join(path_data, \"*.edf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting the features, we are going to perform downsampling and a bandpass filter to keep only the low frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf(path_files[0], preload=False, verbose=False)\n",
    "# Downsample the data to 100 Hz\n",
    "raw.resample(100)\n",
    "# Apply a bandpass filter from 0.3 to 49 Hz\n",
    "raw.filter(0.3, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done, we can use the function `yasa.bandpower()` to extract frequency-related features from the data. Keep in mind that this function expects the data to be fed in epochs form, so we have to transform it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 719 events and 3000 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>Sleep stage N1: 29<br/>Sleep stage N2: 323<br/>Sleep stage W: 366</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>0.000 – 29.990 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Epochs |  718 events (all good), 0 - 29.99 sec, baseline off, ~18 kB, data not loaded,\n",
       " 'Sleep stage N1': 29\n",
       " 'Sleep stage N2': 323\n",
       " 'Sleep stage W': 366>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs, sr = get_epochs(raw, channels=channels, verbose=False)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calculate_bandpower(epochs, # Epochs object or 3D array [Epochs, Channels, Data].\n",
    "                        sf=100, # Sampling frequency of the data.\n",
    "                        ) -> np.array: # Numpy array of shape [Epochs, 6*Channels] representing 6 bands.\n",
    "    \"\"\"Extracts the bandpower per epoch and returns it as an array ready to be fed into a model.\"\"\"\n",
    "    n_channels = len(epochs.ch_names) if isinstance(epochs, mne.epochs.Epochs) else epochs.shape[1]\n",
    "    bandpowers = np.empty(shape=(len(epochs), 6*n_channels))\n",
    "    for i, epoch in enumerate(epochs):\n",
    "        df_bandpower = yasa.bandpower(epoch, sf=sf)\n",
    "        df_bandpower.drop(['TotalAbsPow', 'Relative', 'FreqRes'], axis=1, inplace=True)\n",
    "        df_bandpower = df_bandpower.to_numpy().flatten()\n",
    "        if len(df_bandpower) != bandpowers.shape[-1]: raise ValueError(\"Shape mismatch between calculated features and pre-allocated array.\")\n",
    "        bandpowers[i] = df_bandpower\n",
    "    return bandpowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 0 ns, total: 4.34 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bandpowers = calculate_bandpower(epochs, sf=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the shape of the produced array, we find that we have quite reduced the dimensionality of the problem but, are we still able to classify the different sleep stages with this features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718, 84)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandpowers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model\n",
    "\n",
    "> To check the usability of this features we will train a very simple model performing a random split within the same recording. As we are building our project sequentially, we want to make sure that the things we do are usable as building blocks for the next things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can extract the labels from the `epochs` object we have already created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = epochs.events[:,-1]\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can randomly split our data and train a simple default random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(bandpowers, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "assert X_train.shape[0] == Y_train.shape[0]\n",
    "assert X_test.shape[0] == Y_test.shape[0]\n",
    "assert X_train.shape[1] == X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 423 ms, sys: 3.37 ms, total: 426 ms\n",
      "Wall time: 444 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5972222222222222)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, Y_train), model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed this simple check, we know that the features extracted hold, at least, some useful information that can be used to predict the different sleep stages. We can continue adding complexity to our pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('sleepstaging')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
