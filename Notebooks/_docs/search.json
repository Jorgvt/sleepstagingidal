[
  {
    "objectID": "00b_sanity_checking.html",
    "href": "00b_sanity_checking.html",
    "title": "Sanity check",
    "section": "",
    "text": "As we saw while we were loading the data, there might be some recordings that don’t have the full set of labels we are interested in. To account for this, we are going to obtain the label distribution per recording to be able to differentiate between fully labelled recordings and non-fully labelled recordings.\nWhen going further in our analysis, it can be important to filter out the non-fully labelled recordings, at least, until we have a full working pipeline we are happy with.\n\nimport numpy as np\nimport pandas as pd\nimport yasa\n\nfrom sleepstagingidal.data import *"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data loading",
    "section": "",
    "text": "Stablishing a data-loading pipeline is always a good starting point for starting every project. In this case in particular, we have to be conscious because each .edf file we are going to work with is about 1 Gb, so performing lazy loading might become crucial for the project."
  },
  {
    "objectID": "data.html#loading-.edf-files",
    "href": "data.html#loading-.edf-files",
    "title": "Data loading",
    "section": "Loading .edf files",
    "text": "Loading .edf files\n\nWe will be using mne.io.read_raw_edf.\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nraw = mne.io.read_raw_edf(path_files[0], preload=False)\nraw\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG29.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Omitted 200 annotation(s) that were outside data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        512.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        256.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nNow that we’ve loaded the file, we can downsample it to 100Hz and apply a low-pass filter to the signal:\n\n# Downsample the data to 100 Hz\nraw.resample(100)\n# Apply a bandpass filter from 0.3 to 49 Hz\nraw.filter(0.3, 49)\n\nFiltering raw data in 1 contiguous segment\nSetting up band-pass filter from 0.3 - 49 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 0.30\n- Lower transition bandwidth: 0.30 Hz (-6 dB cutoff frequency: 0.15 Hz)\n- Upper passband edge: 49.00 Hz\n- Upper transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 49.50 Hz)\n- Filter length: 1101 samples (11.010 sec)\n\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.5s finished\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        100.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.30 Hz\n    \n    \n    \n    \n        Lowpass\n        49.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nThe different channels of the signal are available in the attribute .ch_names:\n\nprint(len(raw.ch_names))\nraw.ch_names\n\n50\n\n\n['C3',\n 'C4',\n 'O1',\n 'O2',\n 'A1',\n 'A2',\n 'Cz',\n 'F3',\n 'F4',\n 'F7',\n 'F8',\n 'Fz',\n 'Fp1',\n 'Fp2',\n 'Fpz',\n 'P3',\n 'P4',\n 'Pz',\n 'T3',\n 'T4',\n 'T5',\n 'T6',\n 'LOC',\n 'ROC',\n 'CHIN1',\n 'CHIN2',\n 'ECGL',\n 'ECGR',\n 'LAT1',\n 'LAT2',\n 'RAT1',\n 'RAT2',\n 'CHEST',\n 'ABD',\n 'FLOW',\n 'SNORE',\n 'DIF5',\n 'DIF6',\n 'POS',\n 'DC2',\n 'DC3',\n 'DC4',\n 'DC5',\n 'DC6',\n 'DC7',\n 'DC8',\n 'DC9',\n 'DC10',\n 'OSAT',\n 'PR']\n\n\nWe can extract a subset of the channels by using the methods .pick_channels() and .drop_channels():\n\n# Select a subset of EEG channels\n# raw.pick_channels(['LOC-A2', 'ROC-A1', 'F3-A2', 'C3-A2', 'O1-A2', 'F4-A1', 'C4-A1', 'O2-A1', 'X1', 'X2', 'X3'])"
  },
  {
    "objectID": "data.html#making-use-of-the-annotations-in-the-recordings",
    "href": "data.html#making-use-of-the-annotations-in-the-recordings",
    "title": "Data loading",
    "section": "Making use of the annotations in the recordings",
    "text": "Making use of the annotations in the recordings\nOne of the peculiarities of this kind of data is they include medical annotations. We can access them in the atribute .annotations:\n\nraw.annotations\n\n<Annotations | 2057 segments: Central Apnea (6), EEG arousal (131), ...>\n\n\nWe can index this object to view more information:\n\nraw.annotations[80]\n\nOrderedDict([('onset', 3420.0),\n             ('duration', 30.0),\n             ('description', 'Sleep stage W'),\n             ('orig_time',\n              datetime.datetime(2019, 1, 1, 22, 48, 22, tzinfo=datetime.timezone.utc))])\n\n\nAs we can see, they include all the information needed to split the signal into epochs:\n\nonset: starting time of an epoch.\nduration: duration of the epoch.\ndescription: label set by the medical staff.\norig_time: date when the data was collected.\n\nWe can use a Counter to count the different labels available in the data:\n\ncntr = Counter(raw.annotations.description)\ncntr\n\nCounter({'Montage:PR, Ref': 2,\n         'Start Recording': 1,\n         'Recording Analyzer - Sleep Events': 1,\n         'Recording Analyzer - Auto-Staging': 1,\n         'Recording Analyzer - ECG': 1,\n         'Recording Analyzer - Data Trends': 1,\n         'Video Recording ON': 1,\n         'Impedance at 10 kOhm': 1,\n         'Obstructive Apnea': 200,\n         'Patient Event': 2,\n         'Sleep stage W': 367,\n         'Lights Off': 1,\n         'Started Analyzer - Sleep Events': 1,\n         'Gain/Filter Change': 2,\n         'Oxygen Desaturation': 341,\n         'Oximeter Event': 272,\n         'Limb Movement': 109,\n         'Sleep stage N1': 29,\n         'EEG arousal': 131,\n         'Snoring': 251,\n         'Sleep stage N2': 323,\n         'Talking': 8,\n         'ORINANDO': 2,\n         'Hypopnea': 3,\n         'Central Apnea': 6})\n\n\n\nplt.figure()\nplt.bar(cntr.keys(), cntr.values())\nplt.xticks(rotation=90)\nplt.xlabel(\"Annotation\")\nplt.ylabel(\"Counts\")\nplt.title(\"Count of each annotation\")\nplt.show()\n\n\n\n\nAs we can see, there are a lot of epochs marked with labels we don’t care about. Later on we will see how we can filter the epochs to keep only the ones we care about."
  },
  {
    "objectID": "data.html#extracting-epochs-from-the-full-recordings",
    "href": "data.html#extracting-epochs-from-the-full-recordings",
    "title": "Data loading",
    "section": "Extracting epochs from the full recordings",
    "text": "Extracting epochs from the full recordings\nTo be able to use the data, we need to extract 30s epochs from the full recordings. We can do that easily with the function mne.make_fixed_length_epochs(), but we can make use of the included annotations and use mne.events_from_annotations() and mne.Epochs to directly obtain the epochs paired with their associated label.\n\nmne.events_from_annotations() allows us to filter by regexp, so we will be able to keep only the epochs corresponding to the labels we care about.\n\n\nsource\n\nget_epochs\n\n get_epochs (data:mne.io.edf.edf.RawEDF, channels:List[str]=None)\n\nExtracts labelled epochs from an already loaded raw edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nRawEDF\n\nRaw file loaded with mne.io.read_raw_edf.\n\n\nchannels\ntyping.List[str]\nNone\nList of channels to keep. If None all the channels are kept.\n\n\nReturns\ntyping.Tuple[mne.epochs.Epochs, float]\n\nEpochs object and samplign rate of the signal.\n\n\n\n\nepochs, sr = get_epochs(raw)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nWhen this objects are created without preloading=True, we need to use the method .get_data() to access the data. As this kind of data uses a lot of memmory, the data of each epoch only are loaded when this method is called. We can check the shape of an epoch:\n\nepochs[0].get_data().shape\n\nUsing data from preloaded Raw for 1 events and 3000 original time points ...\n\n\n(1, 50, 3000)\n\n\nThe dimmensions of an epoch are: (1, Number of channels, Data points per channel).\n\nActually, the first dimmension corresponds to the number of epochs loaded, but we only loaded one epoch.\n\nLastly, this object stores two important atributes:\n\n.events: contains the parsed labels in the last dimension.\n.event_id: dictionary mapping the parsed labels to the original labels.\n\n\nepochs.events\n\narray([[ 228000,       0,       3],\n       [ 231000,       0,       3],\n       [ 234000,       0,       3],\n       ...,\n       [2373000,       0,       3],\n       [2376000,       0,       3],\n       [2379000,       0,       3]])\n\n\n\nepochs.event_id\n\n{'Sleep stage N1': 1, 'Sleep stage N2': 2, 'Sleep stage W': 3}\n\n\nComes to our attention that this patient has only been annotated with 3 out of 5 of the possible sleep stages. This might be important to look at to differentiate from full labeled patients and the rest."
  },
  {
    "objectID": "data.html#inter-channel-operations",
    "href": "data.html#inter-channel-operations",
    "title": "Data loading",
    "section": "Inter-channel operations",
    "text": "Inter-channel operations\nReal practitioners don’t actually use the data as we have imported it. They only utilize 9 channels, and perform reference operations between them. We can now reload the data using only this channels to save memory and perform the reference operations:\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\n\nepochs, sr = get_epochs(raw, channels=channels)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nThe reference operations they use are:\n\\[ C3 - \\frac{A1+A2}{2} \\] \\[ C4 - \\frac{A1+A2}{2} \\] \\[ 01 - \\frac{A1+A2}{2} \\] \\[ 02 - \\frac{A1+A2}{2} \\] \\[ LOC - A2 \\] \\[ ROC - A1 \\] \\[ LAT1 - LAT2 \\] \\[ ECGL - ECGR \\] \\[ CHIN1 - CHIN2 \\]\nIn order to mantain the code clean, we can put our data into a dictionary and then perform the operations. But fist we will load all the data in memory:\n\nX = epochs.get_data()\nY = epochs.events[:,-1]\nX.shape, Y.shape\n\nUsing data from preloaded Raw for 718 events and 3000 original time points ...\n\n\n((718, 14, 3000), (718,))\n\n\n\n# We need to expand_dims to keep the channels dim\ndata = {ch:np.expand_dims(X[:,i,:],1) for i, ch in enumerate(channels)}\n\n\nchannel1 = data[\"C3\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel2 = data[\"C4\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel3 = data[\"O1\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel4 = data[\"O2\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel5 = data[\"LOC\"] - data[\"A2\"]\nchannel6 = data[\"ROC\"] - data[\"A1\"]\nchannel7 = data[\"LAT1\"] - data[\"LAT2\"]\nchannel8 = data[\"ECGL\"] - data[\"ECGR\"]\nchannel9 = data[\"CHIN1\"] - data[\"CHIN2\"]\n\nNow we can concatenate the new channels into a new array:\n\nX = np.concatenate([channel1, channel2, channel3, channel4, channel5, channel6, channel7, channel8, channel9],1)\nX.shape\n\n(718, 9, 3000)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sleepstagingidal",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/sleepstagingidal.git\ncd sleepstagingidal\npip install -e ."
  },
  {
    "objectID": "00_Memo/00_data_loading.html",
    "href": "00_Memo/00_data_loading.html",
    "title": "Data loading",
    "section": "",
    "text": "Stablishing a data-loading pipeline is always a good starting point for starting every project. In this case in particular, we have to be conscious because each .edf file we are going to work with is about 1 Gb, so performing lazy loading might become crucial for the project."
  },
  {
    "objectID": "data.html#amount-of-sleep-stages-per-patient",
    "href": "data.html#amount-of-sleep-stages-per-patient",
    "title": "Sanity check",
    "section": "Amount of sleep stages per patient",
    "text": "Amount of sleep stages per patient\nThe main thing we want to check is the amount of different labels that are present in each file: We expect each file to have 5 sleep stages (W, N1, N2, N3, R). This will be an easy thing to obtain using the functions we have previously defined:\n\npath_files[0].split(\"/\")[-1]\n\n'PSG29.edf'\n\n\n\nall_sleep_stages = {}\nfor path_file in track(path_files):\n    file = path_file.split(\"/\")[-1]\n    sleep_stages = get_sleep_stages(path_file)\n    all_sleep_stages[file] = sleep_stages\n\n\n\n\n\n\n\n\n\n\nNow that we have all the stages per recording, we can obtain:\n\nQuantity of stages per recording.\nNumber of different stages per recording.\n\n\nall_quantity = {file:len(stages) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_quantity.keys(), all_quantity.values())\nplt.xticks(rotation=90)\nplt.title(\"Amount of sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"Amouint of stages labelled\")\nplt.show()\n\n\n\n\n\nall_different = {file:len(set(stages)) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_different.keys(), all_different.values())\nplt.xticks(rotation=90)\nplt.title(\"# of different sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"# of different sleep stages\")\nplt.show()"
  },
  {
    "objectID": "data.html#summing-up",
    "href": "data.html#summing-up",
    "title": "Sanity check",
    "section": "Summing up",
    "text": "Summing up\nThanks to this easy and quick exploration we have been able to recognize that the file PSG22.edf might have a problem because no stages are shown, and that files PSG29.edf, PSG10.edf, PSG23.edf, PSG32.edf and PSG28.edf might be problematic because they are missing some of the labels we are interested in. We can end this by creating a, for example, .csv file indicating the files that are completed and the ones that are not, so that we can choose which to load depending on what analysis we want to perform on our data.\n\ndf = pd.DataFrame.from_dict(all_different, orient='index', columns=['DifferentStages'])\ndf.index.set_names(\"File\", inplace=True)\ndf[\"Complete\"] = df.DifferentStages == 5\ndf.head()\n\n\n\n\n\n  \n    \n      \n      DifferentStages\n      Complete\n    \n    \n      File\n      \n      \n    \n  \n  \n    \n      PSG29.edf\n      3\n      False\n    \n    \n      PSG12.edf\n      5\n      True\n    \n    \n      PSG17.edf\n      5\n      True\n    \n    \n      PSG10.edf\n      4\n      False\n    \n    \n      PSG23.edf\n      3\n      False\n    \n  \n\n\n\n\n\ndf.to_csv(\"info.csv\")"
  }
]