[
  {
    "objectID": "feature_extraction.html",
    "href": "feature_extraction.html",
    "title": "Feature extraction",
    "section": "",
    "text": "from multiprocessing.spawn import import_main_path\nimport os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict, Tuple\n\nfrom rich.progress import track\nimport numpy as np\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *"
  },
  {
    "objectID": "feature_extraction.html#amplitude-independent-features",
    "href": "feature_extraction.html#amplitude-independent-features",
    "title": "Feature extraction",
    "section": "Amplitude-independent features",
    "text": "Amplitude-independent features\nIn our case, we have downsampled our data and have 3000 data points per epoch (30s fragment of the recording), but this is a huge amount of features to be fed into a model. Getting even further, we have the problem of calibration and normalization, which is aggravated even more when working with medical data (each hospital may work slightly different). Because of this, we are going to employ features that are independent of the amplitude of the signal: we are going to utilize mainly frequency-related features.\nWe can use the library yasa to perform a basic feature extraction:\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\nBefore extracting the features, we are going to perform downsampling and a bandpass filter to keep only the low frequencies:\n\nsource\n\nread_clean_edf\n\n read_clean_edf (path, resample:int=None,\n                 bandpass:Tuple[float,float]=None, verbose:bool=False)\n\nLoads and (potentially) cleans an .edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\n\n\nPath to an .edf file.\n\n\nresample\nint\nNone\nFrequency (Hz) we want to resample to. If None, no resampling is applied.\n\n\nbandpass\ntyping.Tuple[float, float]\nNone\nTuple (min_freq, max_freq) for the bandpass filter. If None, no bandpass is applied.\n\n\nverbose\nbool\nFalse\nQuantity of information to be shown.\n\n\nReturns\nRawEDF\n\nRaw object with the corresponding cleaning,\n\n\n\n\nraw = read_clean_edf(path_files[0], resample=100, bandpass=(0.3, 49))\nraw\n\nOnce it’s done, we can use the function yasa.bandpower() to extract frequency-related features from the data. Keep in mind that this function expects the data to be fed in epochs form, so we have to transform it first:\n\nepochs, sr = get_epochs(raw, channels=channels, verbose=False)\nepochs\n\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\n\n\n    \n        Number of events\n        718\n    \n    \n        Events\n        \n        Sleep stage N1: 29Sleep stage N2: 323Sleep stage W: 366\n        \n    \n    \n        Time range\n        0.000 – 29.990 sec\n    \n    \n        Baseline\n        off\n    \n\n\n\n\nsource\n\n\ncalculate_bandpower\n\n calculate_bandpower (epochs, sf=100)\n\nExtracts the bandpower per epoch and returns it as an array ready to be fed into a model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\n\n\nEpochs object or 3D array [Epochs, Channels, Data].\n\n\nsf\nint\n100\nSampling frequency of the data.\n\n\nReturns\narray\n\nNumpy array of shape [Epochs, 6*Channels] representing 6 bands.\n\n\n\n\nbandpowers = calculate_bandpower(epochs, sf=sr)\n\nCPU times: user 4.34 s, sys: 0 ns, total: 4.34 s\nWall time: 4.34 s\n\n\nIf we check the shape of the produced array, we find that we have quite reduced the dimensionality of the problem but, are we still able to classify the different sleep stages with this features?\n\nbandpowers.shape\n\n(718, 84)"
  },
  {
    "objectID": "feature_extraction.html#simple-model",
    "href": "feature_extraction.html#simple-model",
    "title": "Feature extraction",
    "section": "Simple model",
    "text": "Simple model\n\nTo check the usability of this features we will train a very simple model performing a random split within the same recording. As we are building our project sequentially, we want to make sure that the things we do are usable as building blocks for the next things.\n\nFirst of all, we can extract the labels from the epochs object we have already created:\n\nlabels = epochs.events[:,-1]\nlabels.shape\n\n(718,)\n\n\nNext, we can randomly split our data and train a simple default random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(bandpowers, labels, test_size=0.2, random_state=42)\n\nassert X_train.shape[0] == Y_train.shape[0]\nassert X_test.shape[0] == Y_test.shape[0]\nassert X_train.shape[1] == X_test.shape[1]\n\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, Y_train)\n\nCPU times: user 423 ms, sys: 3.37 ms, total: 426 ms\nWall time: 444 ms\n\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nmodel.score(X_train, Y_train), model.score(X_test, Y_test)\n\n(1.0, 0.5972222222222222)\n\n\nHaving performed this simple check, we know that the features extracted hold, at least, some useful information that can be used to predict the different sleep stages. We can continue adding complexity to our pipeline."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data loading",
    "section": "",
    "text": "Stablishing a data-loading pipeline is always a good starting point for starting every project. In this case in particular, we have to be conscious because each .edf file we are going to work with is about 1 Gb, so performing lazy loading might become crucial for the project."
  },
  {
    "objectID": "data.html#loading-.edf-files",
    "href": "data.html#loading-.edf-files",
    "title": "Data loading",
    "section": "Loading .edf files",
    "text": "Loading .edf files\n\nWe will be using mne.io.read_raw_edf.\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nraw = mne.io.read_raw_edf(path_files[0], preload=False)\nraw\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG29.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Omitted 200 annotation(s) that were outside data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        512.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        256.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nNow that we’ve loaded the file, we can downsample it to 100Hz and apply a low-pass filter to the signal:\n\n# Downsample the data to 100 Hz\nraw.resample(100)\n# Apply a bandpass filter from 0.3 to 49 Hz\nraw.filter(0.3, 49)\n\nFiltering raw data in 1 contiguous segment\nSetting up band-pass filter from 0.3 - 49 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 0.30\n- Lower transition bandwidth: 0.30 Hz (-6 dB cutoff frequency: 0.15 Hz)\n- Upper passband edge: 49.00 Hz\n- Upper transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 49.50 Hz)\n- Filter length: 1101 samples (11.010 sec)\n\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.5s finished\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        100.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.30 Hz\n    \n    \n    \n    \n        Lowpass\n        49.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nThe different channels of the signal are available in the attribute .ch_names:\n\nprint(len(raw.ch_names))\nraw.ch_names\n\n50\n\n\n['C3',\n 'C4',\n 'O1',\n 'O2',\n 'A1',\n 'A2',\n 'Cz',\n 'F3',\n 'F4',\n 'F7',\n 'F8',\n 'Fz',\n 'Fp1',\n 'Fp2',\n 'Fpz',\n 'P3',\n 'P4',\n 'Pz',\n 'T3',\n 'T4',\n 'T5',\n 'T6',\n 'LOC',\n 'ROC',\n 'CHIN1',\n 'CHIN2',\n 'ECGL',\n 'ECGR',\n 'LAT1',\n 'LAT2',\n 'RAT1',\n 'RAT2',\n 'CHEST',\n 'ABD',\n 'FLOW',\n 'SNORE',\n 'DIF5',\n 'DIF6',\n 'POS',\n 'DC2',\n 'DC3',\n 'DC4',\n 'DC5',\n 'DC6',\n 'DC7',\n 'DC8',\n 'DC9',\n 'DC10',\n 'OSAT',\n 'PR']\n\n\nWe can extract a subset of the channels by using the methods .pick_channels() and .drop_channels():\n\n# Select a subset of EEG channels\n# raw.pick_channels(['LOC-A2', 'ROC-A1', 'F3-A2', 'C3-A2', 'O1-A2', 'F4-A1', 'C4-A1', 'O2-A1', 'X1', 'X2', 'X3'])"
  },
  {
    "objectID": "data.html#making-use-of-the-annotations-in-the-recordings",
    "href": "data.html#making-use-of-the-annotations-in-the-recordings",
    "title": "Data loading",
    "section": "Making use of the annotations in the recordings",
    "text": "Making use of the annotations in the recordings\nOne of the peculiarities of this kind of data is they include medical annotations. We can access them in the atribute .annotations:\n\nraw.annotations\n\n<Annotations | 2057 segments: Central Apnea (6), EEG arousal (131), ...>\n\n\nWe can index this object to view more information:\n\nraw.annotations[80]\n\nOrderedDict([('onset', 3420.0),\n             ('duration', 30.0),\n             ('description', 'Sleep stage W'),\n             ('orig_time',\n              datetime.datetime(2019, 1, 1, 22, 48, 22, tzinfo=datetime.timezone.utc))])\n\n\nAs we can see, they include all the information needed to split the signal into epochs:\n\nonset: starting time of an epoch.\nduration: duration of the epoch.\ndescription: label set by the medical staff.\norig_time: date when the data was collected.\n\nWe can use a Counter to count the different labels available in the data:\n\ncntr = Counter(raw.annotations.description)\ncntr\n\nCounter({'Montage:PR, Ref': 2,\n         'Start Recording': 1,\n         'Recording Analyzer - Sleep Events': 1,\n         'Recording Analyzer - Auto-Staging': 1,\n         'Recording Analyzer - ECG': 1,\n         'Recording Analyzer - Data Trends': 1,\n         'Video Recording ON': 1,\n         'Impedance at 10 kOhm': 1,\n         'Obstructive Apnea': 200,\n         'Patient Event': 2,\n         'Sleep stage W': 367,\n         'Lights Off': 1,\n         'Started Analyzer - Sleep Events': 1,\n         'Gain/Filter Change': 2,\n         'Oxygen Desaturation': 341,\n         'Oximeter Event': 272,\n         'Limb Movement': 109,\n         'Sleep stage N1': 29,\n         'EEG arousal': 131,\n         'Snoring': 251,\n         'Sleep stage N2': 323,\n         'Talking': 8,\n         'ORINANDO': 2,\n         'Hypopnea': 3,\n         'Central Apnea': 6})\n\n\n\nplt.figure()\nplt.bar(cntr.keys(), cntr.values())\nplt.xticks(rotation=90)\nplt.xlabel(\"Annotation\")\nplt.ylabel(\"Counts\")\nplt.title(\"Count of each annotation\")\nplt.show()\n\n\n\n\nAs we can see, there are a lot of epochs marked with labels we don’t care about. Later on we will see how we can filter the epochs to keep only the ones we care about."
  },
  {
    "objectID": "data.html#extracting-epochs-from-the-full-recordings",
    "href": "data.html#extracting-epochs-from-the-full-recordings",
    "title": "Data loading",
    "section": "Extracting epochs from the full recordings",
    "text": "Extracting epochs from the full recordings\nTo be able to use the data, we need to extract 30s epochs from the full recordings. We can do that easily with the function mne.make_fixed_length_epochs(), but we can make use of the included annotations and use mne.events_from_annotations() and mne.Epochs to directly obtain the epochs paired with their associated label.\n\nmne.events_from_annotations() allows us to filter by regexp, so we will be able to keep only the epochs corresponding to the labels we care about.\n\n\nsource\n\nget_epochs\n\n get_epochs (data:mne.io.edf.edf.RawEDF, channels:List[str]=None,\n             verbose:bool=False)\n\nExtracts labelled epochs from an already loaded raw edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nRawEDF\n\nRaw file loaded with mne.io.read_raw_edf.\n\n\nchannels\ntyping.List[str]\nNone\nList of channels to keep. If None all the channels are kept.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\ntyping.Tuple[mne.epochs.Epochs, float]\n\nEpochs object and samplign rate of the signal.\n\n\n\n\nsource\n\n\nget_epochs_from_path\n\n get_epochs_from_path (path:str, channels:List[str]=None,\n                       verbose:bool=False)\n\nExtracts labelled epochs from an .edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nPath to an .edf file.\n\n\nchannels\ntyping.List[str]\nNone\nList of channels to keep. If None all the channels are kept.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\ntyping.Tuple[mne.epochs.Epochs, float]\n\nEpochs object and samplign rate of the signal.\n\n\n\n\nepochs, sr = get_epochs(raw)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nWhen this objects are created without preloading=True, we need to use the method .get_data() to access the data. As this kind of data uses a lot of memmory, the data of each epoch only are loaded when this method is called. We can check the shape of an epoch:\n\nepochs[0].get_data().shape\n\nUsing data from preloaded Raw for 1 events and 3000 original time points ...\n\n\n(1, 50, 3000)\n\n\nThe dimmensions of an epoch are: (1, Number of channels, Data points per channel).\n\nActually, the first dimmension corresponds to the number of epochs loaded, but we only loaded one epoch.\n\nLastly, this object stores two important atributes:\n\n.events: contains the parsed labels in the last dimension.\n.event_id: dictionary mapping the parsed labels to the original labels.\n\n\nepochs.events\n\narray([[ 228000,       0,       3],\n       [ 231000,       0,       3],\n       [ 234000,       0,       3],\n       ...,\n       [2373000,       0,       3],\n       [2376000,       0,       3],\n       [2379000,       0,       3]])\n\n\n\nepochs.event_id\n\n{'Sleep stage N1': 1, 'Sleep stage N2': 2, 'Sleep stage W': 3}\n\n\nComes to our attention that this patient has only been annotated with 3 out of 5 of the possible sleep stages. This might be important to look at to differentiate from full labeled patients and the rest."
  },
  {
    "objectID": "data.html#inter-channel-operations",
    "href": "data.html#inter-channel-operations",
    "title": "Data loading",
    "section": "Inter-channel operations",
    "text": "Inter-channel operations\nReal practitioners don’t actually use the data as we have imported it. They only utilize 9 channels, and perform reference operations between them. We can now reload the data using only this channels to save memory and perform the reference operations:\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\n\nepochs, sr = get_epochs(raw, channels=channels)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nThe reference operations they use are:\n\\[ C3 - \\frac{A1+A2}{2} \\] \\[ C4 - \\frac{A1+A2}{2} \\] \\[ 01 - \\frac{A1+A2}{2} \\] \\[ 02 - \\frac{A1+A2}{2} \\] \\[ LOC - A2 \\] \\[ ROC - A1 \\] \\[ LAT1 - LAT2 \\] \\[ ECGL - ECGR \\] \\[ CHIN1 - CHIN2 \\]\nIn order to mantain the code clean, we can put our data into a dictionary and then perform the operations. But fist we will load all the data in memory:\n\nX = epochs.get_data()\nY = epochs.events[:,-1]\nX.shape, Y.shape\n\nUsing data from preloaded Raw for 718 events and 3000 original time points ...\n\n\n((718, 14, 3000), (718,))\n\n\n\n# We need to expand_dims to keep the channels dim\ndata = {ch:np.expand_dims(X[:,i,:],1) for i, ch in enumerate(channels)}\n\n\nchannel1 = data[\"C3\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel2 = data[\"C4\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel3 = data[\"O1\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel4 = data[\"O2\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel5 = data[\"LOC\"] - data[\"A2\"]\nchannel6 = data[\"ROC\"] - data[\"A1\"]\nchannel7 = data[\"LAT1\"] - data[\"LAT2\"]\nchannel8 = data[\"ECGL\"] - data[\"ECGR\"]\nchannel9 = data[\"CHIN1\"] - data[\"CHIN2\"]\n\nNow we can concatenate the new channels into a new array:\n\nX = np.concatenate([channel1, channel2, channel3, channel4, channel5, channel6, channel7, channel8, channel9],1)\nX.shape\n\n(718, 9, 3000)"
  },
  {
    "objectID": "dataa.html",
    "href": "dataa.html",
    "title": "Sanity check",
    "section": "",
    "text": "As we saw while we were loading the data, there might be some recordings that don’t have the full set of labels we are interested in. To account for this, we are going to obtain the label distribution per recording to be able to differentiate between fully labelled recordings and non-fully labelled recordings.\nWhen going further in our analysis, it can be important to filter out the non-fully labelled recordings, at least, until we have a full working pipeline we are happy with.\n\nimport os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict\n\nfrom rich.progress import track\nimport numpy as np\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sleepstagingidal.data import *\n\nThe steps we have to follow are:\n\nLoad the raw .edf file.\nExtract transform the Annotations into Events and filter them using regex.\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nraw = mne.io.read_raw_edf(path_files[0], preload=False)\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG29.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n\nevents, events_id = mne.events_from_annotations(raw, regexp='Sleep stage [A-Z]\\d*')\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\n\n\nWhen obtaining events from annotations, the annotations are encoded in sparse notation. To ease our analysis we can define a couple of functions to turn them back easily into human readable format again:\n\nsource\n\n\n\n swap_dict (dictionary:Dict)\n\nTurns the keys into values and the values into keys.\n\n\n\n\nType\nDetails\n\n\n\n\ndictionary\ntyping.Dict\ndictionary to be swapped,\n\n\nReturns\ntyping.Dict\nSwapped dictionary\n\n\n\n\nsource\n\n\n\n\n map_events (events, mapping:Dict)\n\nTurns an encoded representation of the annotations into a human readable one using the corresponding mapping dictionary\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nevents\n\nevents array obtained from mne.events_from_annotations().\n\n\nmapping\ntyping.Dict\ndictionary mapping from encoded annotations to readable.\n\n\nReturns\ntyping.List[str]\nmapped events in human readable format.\n\n\n\n\nassert len(events[:,-1]) == len(map_events(events, swap_dict(events_id)))\n\nTo wrap it up, let’s define a function that takes as input a path to a file and outputs its event annotations in human readable form:\n\nsource\n\n\n\n\n get_sleep_stages (path:str, verbose:bool=False)\n\nLoads an .edf file and extracts the sleep stage labels in human readable form.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nPath to an .edf file.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\ntyping.List[str]\n\nannotations in human readable form.\n\n\n\n\nsleep_stages = get_sleep_stages(path_files[0])\nassert len(sleep_stages) == len(events[:,-1])\n\nOnce we have all the interest labels from a file extracted, we can use a Counter to obtain its distribution:\n\nCounter(sleep_stages)\n\nCounter({'Sleep stage W': 367, 'Sleep stage N1': 29, 'Sleep stage N2': 323})"
  },
  {
    "objectID": "dataa.html#amount-of-sleep-stages-per-patient",
    "href": "dataa.html#amount-of-sleep-stages-per-patient",
    "title": "Sanity check",
    "section": "Amount of sleep stages per patient",
    "text": "Amount of sleep stages per patient\nThe main thing we want to check is the amount of different labels that are present in each file: We expect each file to have 5 sleep stages (W, N1, N2, N3, R). This will be an easy thing to obtain using the functions we have previously defined:\n\npath_files[0].split(\"/\")[-1]\n\n'PSG29.edf'\n\n\n\nall_sleep_stages = {}\nfor path_file in track(path_files):\n    file = path_file.split(\"/\")[-1]\n    sleep_stages = get_sleep_stages(path_file)\n    all_sleep_stages[file] = sleep_stages\n\n\n\n\n\n\n\n\n\n\nNow that we have all the stages per recording, we can obtain:\n\nQuantity of stages per recording.\nNumber of different stages per recording.\n\n\nall_quantity = {file:len(stages) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_quantity.keys(), all_quantity.values())\nplt.xticks(rotation=90)\nplt.title(\"Amount of sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"Amouint of stages labelled\")\nplt.show()\n\n\n\n\n\nall_different = {file:len(set(stages)) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_different.keys(), all_different.values())\nplt.xticks(rotation=90)\nplt.title(\"# of different sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"# of different sleep stages\")\nplt.show()"
  },
  {
    "objectID": "dataa.html#summing-up",
    "href": "dataa.html#summing-up",
    "title": "Sanity check",
    "section": "Summing up",
    "text": "Summing up\nThanks to this easy and quick exploration we have been able to recognize that the file PSG22.edf might have a problem because no stages are shown, and that files PSG29.edf, PSG10.edf, PSG23.edf, PSG32.edf and PSG28.edf might be problematic because they are missing some of the labels we are interested in. We can end this by creating a, for example, .csv file indicating the files that are completed and the ones that are not, so that we can choose which to load depending on what analysis we want to perform on our data.\n\ndf = pd.DataFrame.from_dict(all_different, orient='index', columns=['DifferentStages'])\ndf.index.set_names(\"File\", inplace=True)\ndf[\"Complete\"] = df.DifferentStages == 5\ndf.head()\n\n\n\n\n\n  \n    \n      \n      DifferentStages\n      Complete\n    \n    \n      File\n      \n      \n    \n  \n  \n    \n      PSG29.edf\n      3\n      False\n    \n    \n      PSG12.edf\n      5\n      True\n    \n    \n      PSG17.edf\n      5\n      True\n    \n    \n      PSG10.edf\n      4\n      False\n    \n    \n      PSG23.edf\n      3\n      False\n    \n  \n\n\n\n\n\ndf.to_csv(\"info.csv\")"
  },
  {
    "objectID": "01_Experiments/00_basic_features_own_patient.html",
    "href": "01_Experiments/00_basic_features_own_patient.html",
    "title": "Using basic features within the same patient",
    "section": "",
    "text": "import os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict\n\nfrom rich.progress import track\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *\nfrom sleepstagingidal.feature_extraction import *"
  },
  {
    "objectID": "01_Experiments/00_basic_features_own_patient.html#looping-through-patients",
    "href": "01_Experiments/00_basic_features_own_patient.html#looping-through-patients",
    "title": "Using basic features within the same patient",
    "section": "Looping through patients",
    "text": "Looping through patients\n\nAs we only want to perform a very basic check, we are going to be looping through all the patients.\n\n\nresults = {}\n\nfor path in track(path_files):\n    file_name = path.split(\"/\")[-1]\n    raw = read_clean_edf(path, resample=100, bandpass=(0.3, 49))\n    epochs, sr = get_epochs(raw, channels=channels)\n    bandpowers = calculate_bandpower(epochs, sf=sr)\n    labels = epochs.events[:,-1]\n    results_cv = cross_validate(RandomForestClassifier(random_state=42), bandpowers, labels)\n    results[file_name] = results_cv['test_score']\n\nWe can put the logged results into a DataFrame and save them as .csv to avoid having to repeat the experiment:\n\ndf = pd.DataFrame.from_dict(results, orient='index')\ndf.index.set_names(\"File\", inplace=True)\ndf['Mean'] = df.mean(axis=1)\ndf['Std'] = df.std(axis=1)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      Mean\n      Std\n    \n    \n      File\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      PSG29.edf\n      0.548611\n      0.472222\n      0.444444\n      0.321678\n      0.517483\n      0.460888\n      0.078328\n    \n    \n      PSG12.edf\n      0.411765\n      0.392157\n      0.614379\n      0.398693\n      0.473684\n      0.458136\n      0.083295\n    \n    \n      PSG17.edf\n      0.349693\n      0.561728\n      0.537037\n      0.506173\n      0.209877\n      0.432902\n      0.133771\n    \n    \n      PSG10.edf\n      0.657895\n      0.801325\n      0.629139\n      0.344371\n      0.576159\n      0.601778\n      0.148749\n    \n    \n      PSG23.edf\n      0.873418\n      0.815287\n      0.821656\n      0.878981\n      0.605096\n      0.798887\n      0.100312\n    \n  \n\n\n\n\n\ndf.to_csv(\"Results/00_basic_features_own_patient.csv\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sleepstagingidal",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/sleepstagingidal.git\ncd sleepstagingidal\npip install -e ."
  }
]