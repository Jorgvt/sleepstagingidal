[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sleepstagingidal",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/sleepstagingidal.git\ncd sleepstagingidal\npip install -e ."
  },
  {
    "objectID": "00_Sanity_Checking/checking_bad_recordings.html",
    "href": "00_Sanity_Checking/checking_bad_recordings.html",
    "title": "Checking bad recordings",
    "section": "",
    "text": "Doesn’t have any of the labels of interest.\n\nIn our previous inspection we found that this recording doesn’t have any of the labels of interest, so we are going to check which labels has to see if they have different names or if anything is wrong with it.\n\nraw = mne.io.read_raw_edf(os.path.join(path_data, \"PSG22.edf\"), preload=False)\nraw\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG22.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2021  22:07:10 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        51 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        512.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        256.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG22.edf\n    \n    \n    \n        Duration\n        07:56:35 (HH:MM:SS)\n    \n\n\n\n\nraw.annotations\n\n<Annotations | 0 segments>\n\n\nSo it looks like the PSG22.edf doesn’t have any annotations attached to it. That’s why we weren’t getting any interest labels."
  },
  {
    "objectID": "00_Sanity_Checking/sanity_checking.html",
    "href": "00_Sanity_Checking/sanity_checking.html",
    "title": "Sanity check",
    "section": "",
    "text": "As we saw while we were loading the data, there might be some recordings that don’t have the full set of labels we are interested in. To account for this, we are going to obtain the label distribution per recording to be able to differentiate between fully labelled recordings and non-fully labelled recordings.\nWhen going further in our analysis, it can be important to filter out the non-fully labelled recordings, at least, until we have a full working pipeline we are happy with.\n\nimport os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict\n\nfrom rich.progress import track\nimport numpy as np\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sleepstagingidal.data import *\n\nThe steps we have to follow are:\n\nLoad the raw .edf file.\nExtract transform the Annotations into Events and filter them using regex.\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nraw = mne.io.read_raw_edf(path_files[0], preload=False)\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG29.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n\nevents, events_id = mne.events_from_annotations(raw, regexp='Sleep stage [A-Z]\\d*')\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\n\n\nWhen obtaining events from annotations, the annotations are encoded in sparse notation. To ease our analysis we can define a couple of functions to turn them back easily into human readable format again:\n\nsource\n\n\n\n swap_dict (dictionary:Dict)\n\nTurns the keys into values and the values into keys.\n\n\n\n\nType\nDetails\n\n\n\n\ndictionary\nDict\ndictionary to be swapped,\n\n\nReturns\nDict\nSwapped dictionary\n\n\n\n\nsource\n\n\n\n\n map_events (events, mapping:Dict)\n\nTurns an encoded representation of the annotations into a human readable one using the corresponding mapping dictionary\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nevents\n\nevents array obtained from mne.events_from_annotations().\n\n\nmapping\nDict\ndictionary mapping from encoded annotations to readable.\n\n\nReturns\nList\nmapped events in human readable format.\n\n\n\n\nassert len(events[:,-1]) == len(map_events(events, swap_dict(events_id)))\n\nTo wrap it up, let’s define a function that takes as input a path to a file and outputs its event annotations in human readable form:\n\nsource\n\n\n\n\n get_sleep_stages (path:str, verbose:bool=False)\n\nLoads an .edf file and extracts the sleep stage labels in human readable form.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nPath to an .edf file.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\nList\n\nannotations in human readable form.\n\n\n\n\nsleep_stages = get_sleep_stages(path_files[0])\nassert len(sleep_stages) == len(events[:,-1])\n\nOnce we have all the interest labels from a file extracted, we can use a Counter to obtain its distribution:\n\nCounter(sleep_stages)\n\nCounter({'Sleep stage W': 367, 'Sleep stage N1': 29, 'Sleep stage N2': 323})"
  },
  {
    "objectID": "00_Sanity_Checking/sanity_checking.html#amount-of-sleep-stages-per-patient",
    "href": "00_Sanity_Checking/sanity_checking.html#amount-of-sleep-stages-per-patient",
    "title": "Sanity check",
    "section": "Amount of sleep stages per patient",
    "text": "Amount of sleep stages per patient\nThe main thing we want to check is the amount of different labels that are present in each file: We expect each file to have 5 sleep stages (W, N1, N2, N3, R). This will be an easy thing to obtain using the functions we have previously defined:\n\npath_files[0].split(\"/\")[-1]\n\n'PSG29.edf'\n\n\n\nall_sleep_stages = {}\nfor path_file in track(path_files):\n    file = path_file.split(\"/\")[-1]\n    sleep_stages = get_sleep_stages(path_file)\n    all_sleep_stages[file] = sleep_stages\n\n\n\n\n\n\n\n\n\n\nNow that we have all the stages per recording, we can obtain:\n\nQuantity of stages per recording.\nNumber of different stages per recording.\n\n\nall_quantity = {file:len(stages) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_quantity.keys(), all_quantity.values())\nplt.xticks(rotation=90)\nplt.title(\"Amount of sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"Amouint of stages labelled\")\nplt.show()\n\n\n\n\n\nall_different = {file:len(set(stages)) for file, stages in all_sleep_stages.items()}\n\n\nplt.figure()\nplt.bar(all_different.keys(), all_different.values())\nplt.xticks(rotation=90)\nplt.title(\"# of different sleep stages per recording\")\nplt.xlabel(\"Recording\")\nplt.ylabel(\"# of different sleep stages\")\nplt.show()"
  },
  {
    "objectID": "00_Sanity_Checking/sanity_checking.html#summing-up",
    "href": "00_Sanity_Checking/sanity_checking.html#summing-up",
    "title": "Sanity check",
    "section": "Summing up",
    "text": "Summing up\nThanks to this easy and quick exploration we have been able to recognize that the file PSG22.edf might have a problem because no stages are shown, and that files PSG29.edf, PSG10.edf, PSG23.edf, PSG32.edf and PSG28.edf might be problematic because they are missing some of the labels we are interested in. We can end this by creating a, for example, .csv file indicating the files that are completed and the ones that are not, so that we can choose which to load depending on what analysis we want to perform on our data.\n\ndf = pd.DataFrame.from_dict(all_different, orient='index', columns=['DifferentStages'])\ndf.index.set_names(\"File\", inplace=True)\ndf[\"Complete\"] = df.DifferentStages == 5\ndf.head()\n\n\n\n\n\n  \n    \n      \n      DifferentStages\n      Complete\n    \n    \n      File\n      \n      \n    \n  \n  \n    \n      PSG29.edf\n      3\n      False\n    \n    \n      PSG12.edf\n      5\n      True\n    \n    \n      PSG17.edf\n      5\n      True\n    \n    \n      PSG10.edf\n      4\n      False\n    \n    \n      PSG23.edf\n      3\n      False\n    \n  \n\n\n\n\n\ndf.to_csv(\"info.csv\")"
  },
  {
    "objectID": "00_Sanity_Checking/checking_annotations.html",
    "href": "00_Sanity_Checking/checking_annotations.html",
    "title": "Checking annotations",
    "section": "",
    "text": "path_files = glob(os.path.join(path_data, \"*.edf\"))\nlen(path_files)\n\n36\n\n\n\nraws = [mne.io.read_raw_edf(path, preload=False, verbose=False) for path in path_files]\n\n\nanns = np.concatenate([raw.annotations.description for raw in raws])\nlen(anns)\n\n70822\n\n\nIt’s easier to understand the information if we represent it visually:\n\ncntr = Counter(anns)\nplt.figure(figsize=(15,6))\nplt.bar(cntr.keys(), cntr.values())\nplt.xticks(rotation=90)\nplt.xlabel(\"Annotation\")\nplt.ylabel(\"Counts\")\nplt.title(\"Count of each annotation\")\nplt.show()\n\n\n\n\nIt can be usefull as well to store this information as a .csv:\n\ndf_anns = pd.DataFrame.from_dict(cntr, orient='index', columns=['Counts'])\ndf_anns.index.set_names('Annotation', inplace=True)\ndf_anns.head()\n\n\n\n\n\n  \n    \n      \n      Counts\n    \n    \n      Annotation\n      \n    \n  \n  \n    \n      Montage:PR, Ref\n      56\n    \n    \n      Start Recording\n      37\n    \n    \n      Recording Analyzer - Sleep Events\n      36\n    \n    \n      Recording Analyzer - Auto-Staging\n      36\n    \n    \n      Recording Analyzer - ECG\n      36\n    \n  \n\n\n\n\n\ndf_anns.to_csv(\"annotations.csv\")"
  },
  {
    "objectID": "02_Exporting_features/basic_features.html",
    "href": "02_Exporting_features/basic_features.html",
    "title": "Exporting basic features",
    "section": "",
    "text": "Resample the data.\nBandpass (0.3, 49) Hz.\nExtract bandpowers.\n\nThen we will be saving them in a .csv file to ease the experimentation.\n\nimport os\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nfrom rich.progress import track\n\nimport yasa\nimport mne\n\nfrom fastcore.foundation import L\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *\nfrom sleepstagingidal.dataa import swap_dict\nfrom sleepstagingidal.feature_extraction import *\nfrom sleepstagingidal.feature_extraction import get_trainable_from_patients\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\nThe first step is going to be loading the different .edf files an processing them:\n\npatients = L([read_clean_edf(path, resample=100, bandpass=(0.3, 49)) for path in track(path_files, description=\"Pre-processing recordings\")])\n\n\n\n\n\n\n\n\n\n\nAs we want to the extracted features to be as flexible as possible we are going to store each entry with indicating the corresponding patient, so that we can perform different forms of cross-validation with this data without having to recalculate it:\n\nfor i, patient in enumerate(patients):\n    name = patient.filenames[0].split(\"/\")[-1]\n    try:\n        features, labels = get_trainable_from_patients([patient], channels=channels, feature_extraction_fn=calculate_bandpower)\n    except:\n        continue\n    if i == 0:\n        df_total = pd.DataFrame(features)\n        df_total[\"Label\"] = labels\n        df_total[\"Patient\"] = name\n    else:\n        df_temp = pd.DataFrame(features)\n        df_temp[\"Label\"] = labels\n        df_temp[\"Patient\"] = name\n        df_total = pd.concat([df_total, df_temp])\n\n\ndf_total.shape\n\n(27680, 86)\n\n\n\ndf_total.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      76\n      77\n      78\n      79\n      80\n      81\n      82\n      83\n      Label\n      Patient\n    \n  \n  \n    \n      0\n      0.495066\n      0.112756\n      0.112814\n      0.064382\n      0.165632\n      0.049350\n      0.362000\n      0.116360\n      0.171950\n      0.159524\n      ...\n      0.064126\n      0.025867\n      0.687305\n      0.084597\n      0.083275\n      0.047782\n      0.070681\n      0.026361\n      Sleep stage W\n      PSG29.edf\n    \n    \n      1\n      0.465074\n      0.117853\n      0.177654\n      0.101286\n      0.104765\n      0.033368\n      0.347451\n      0.205411\n      0.208832\n      0.093656\n      ...\n      0.050156\n      0.026275\n      0.811640\n      0.054457\n      0.050954\n      0.026253\n      0.037776\n      0.018920\n      Sleep stage W\n      PSG29.edf\n    \n    \n      2\n      0.493321\n      0.083727\n      0.160615\n      0.093394\n      0.127338\n      0.041605\n      0.509627\n      0.099740\n      0.174744\n      0.106081\n      ...\n      0.057003\n      0.022188\n      0.761949\n      0.065250\n      0.071764\n      0.036249\n      0.047086\n      0.017702\n      Sleep stage W\n      PSG29.edf\n    \n    \n      3\n      0.496456\n      0.078696\n      0.145985\n      0.073315\n      0.168728\n      0.036820\n      0.415612\n      0.150171\n      0.162990\n      0.084801\n      ...\n      0.102050\n      0.051728\n      0.657698\n      0.107005\n      0.083062\n      0.039670\n      0.079945\n      0.032621\n      Sleep stage W\n      PSG29.edf\n    \n    \n      4\n      0.499096\n      0.090408\n      0.128249\n      0.119394\n      0.126275\n      0.036577\n      0.355009\n      0.114010\n      0.227973\n      0.142860\n      ...\n      0.064617\n      0.030325\n      0.728514\n      0.062010\n      0.082649\n      0.035992\n      0.059911\n      0.030924\n      Sleep stage W\n      PSG29.edf\n    \n  \n\n5 rows × 86 columns\n\n\n\n\ndf_total.to_csv(\"basic_features.csv\")"
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "Validating the results",
    "section": "",
    "text": "import os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom fastcore.basics import patch\nfrom fastcore.foundation import L\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *\nfrom sleepstagingidal.dataa import swap_dict\nfrom sleepstagingidal.feature_extraction import *"
  },
  {
    "objectID": "cross_validation.html#patient-fold",
    "href": "cross_validation.html#patient-fold",
    "title": "Validating the results",
    "section": "Patient-Fold",
    "text": "Patient-Fold\nBefore trying a lot of different configurations for the models or different feature extraction techniques, it’s crucial to set up a truthful way of knowing how are this changes affecting our results. Because of that, we’re going to lay out the fundation of our validation pipeline: the Patient-Fold.\nBy similarity with traditional K-Fold, we are going to separate all the recordings we have and, iteratively, train with some of them while testing with a different set. This way of performing cross-validation will give us a good estimate on the inter-patient generalization capability of the model.\n\nfrom sklearn.model_selection import KFold\n\n\nsource\n\nPatientFold\n\n PatientFold (path_files:List[str], n_splits:int, random_state:int)\n\nManager to perform the so-called PatientFold.\n\n\n\n\nType\nDetails\n\n\n\n\npath_files\nList\nPath to the .edf files we want to use.\n\n\nn_splits\nint\nNumber of folds to use.\n\n\nrandom_state\nint\nRandom seed for reproducibility\n\n\n\nLoading and preprocessing the raw .edf files takes quite a lot of time, so it can be very convenient to separate that part from the cross-validation part. Keep in mind that we can do this without collapsin the memory from the server because the loaded files themselves load the data in a lazy way. The best way to ensure that the loading and preprocessing is done only once is to use a property:\n\nsource\n\n\nPatientFold.patients\n\n PatientFold.patients ()\n\nEnsures that the .edf files are only loaded and preprocessed once.\nWe know that different recordings may have different encodings for the same sleep stage, so we should be unifying them before joining data from different recordings. The easiest way to do it is turning them into their human-readable representation, and encode all of them together to ensure that all of them are encoded in the same way.\nAnd finally, we can build a simple function to build the appropriate input data and its labels from a set of patients loaded:\nWe want the process to be as streamlined as possible, so we can implement a .fit() method to quickly perform the Patient-Fold with any estimator:\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nsource\n\n\nPatientFold.fit\n\n PatientFold.fit (estimator, **kwargs)\n\nPerforms the cross-validation loop by training the estimator on the different folds and returns the results.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nestimator\nAny object implementing a .fit() method to be crossvalidated. Must not be instantiated.\n\n\nkwargs\n\n\n\n\n\npf = PatientFold(path_files=path_files[:2],\n                 n_splits=len(path_files[:2]),\n                 random_state=42)\n\n\npf.fit(RandomForestClassifier)\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data from preloaded Raw for 765 events and 3000 original time points ...\n\n\n\n1 bad epochs dropped\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n\n\n\n1 bad epochs dropped\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n\n\n\n1 bad epochs dropped\n\n\n\n\n\n\n\n\n\n\n\n\nUsing data from preloaded Raw for 765 events and 3000 original time points ...\n\n\n\n1 bad epochs dropped\n\n\n\n\n\n\n\n\n\nValueError: y contains previously unseen labels: 'Sleep stage N3'"
  },
  {
    "objectID": "01_Experiments/basic_features.html",
    "href": "01_Experiments/basic_features.html",
    "title": "[REM/NO REM] Pre-extracted basic features",
    "section": "",
    "text": "In this quick experiment we are going to utilize the pre-extracted features to perform a classification between REM and No REM stages."
  },
  {
    "objectID": "01_Experiments/basic_features.html#load-and-filter-the-data",
    "href": "01_Experiments/basic_features.html#load-and-filter-the-data",
    "title": "[REM/NO REM] Pre-extracted basic features",
    "section": "Load and filter the data",
    "text": "Load and filter the data\nThe first thing we have to do is loading the features we have previously extracted:\n\ndf = pd.read_csv(path_data, index_col=0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      ...\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n      60\n      61\n      62\n      63\n      64\n      65\n      66\n      67\n      68\n      69\n      70\n      71\n      72\n      73\n      74\n      75\n      76\n      77\n      78\n      79\n      80\n      81\n      82\n      83\n      Label\n      Patient\n    \n  \n  \n    \n      0\n      0.495066\n      0.112756\n      0.112814\n      0.064382\n      0.165632\n      0.049350\n      0.362000\n      0.116360\n      0.171950\n      0.159524\n      0.138463\n      0.051704\n      0.543319\n      0.110169\n      0.118650\n      0.060939\n      0.117710\n      0.049213\n      0.476496\n      0.148164\n      0.147317\n      0.106843\n      0.090631\n      0.030549\n      0.531814\n      0.170641\n      0.118979\n      0.070764\n      0.080622\n      0.027180\n      0.475519\n      0.188196\n      0.133852\n      0.095128\n      0.080400\n      0.026905\n      0.527519\n      0.122290\n      0.133960\n      0.065811\n      ...\n      0.089403\n      0.032273\n      0.200669\n      0.291248\n      0.213199\n      0.142235\n      0.147633\n      0.005017\n      0.200751\n      0.292008\n      0.212910\n      0.141983\n      0.147257\n      0.005091\n      0.197748\n      0.173513\n      0.231294\n      0.181297\n      0.190539\n      0.025610\n      0.178459\n      0.252283\n      0.216593\n      0.158911\n      0.186102\n      0.007653\n      0.721550\n      0.071222\n      0.073576\n      0.043658\n      0.064126\n      0.025867\n      0.687305\n      0.084597\n      0.083275\n      0.047782\n      0.070681\n      0.026361\n      Sleep stage W\n      PSG29.edf\n    \n    \n      1\n      0.465074\n      0.117853\n      0.177654\n      0.101286\n      0.104765\n      0.033368\n      0.347451\n      0.205411\n      0.208832\n      0.093656\n      0.111676\n      0.032974\n      0.630552\n      0.109801\n      0.111556\n      0.065251\n      0.059425\n      0.023416\n      0.480078\n      0.183739\n      0.148267\n      0.069932\n      0.085786\n      0.032198\n      0.616104\n      0.143776\n      0.113066\n      0.057664\n      0.053632\n      0.015759\n      0.545867\n      0.166890\n      0.141956\n      0.065644\n      0.060475\n      0.019168\n      0.531884\n      0.154121\n      0.136999\n      0.072946\n      ...\n      0.092982\n      0.030478\n      0.206725\n      0.284727\n      0.203176\n      0.151529\n      0.147839\n      0.006005\n      0.205184\n      0.285613\n      0.203717\n      0.151011\n      0.148545\n      0.005929\n      0.357687\n      0.172565\n      0.202435\n      0.112405\n      0.134012\n      0.020895\n      0.186969\n      0.243792\n      0.201004\n      0.168602\n      0.191435\n      0.008197\n      0.756929\n      0.068393\n      0.065536\n      0.032712\n      0.050156\n      0.026275\n      0.811640\n      0.054457\n      0.050954\n      0.026253\n      0.037776\n      0.018920\n      Sleep stage W\n      PSG29.edf\n    \n    \n      2\n      0.493321\n      0.083727\n      0.160615\n      0.093394\n      0.127338\n      0.041605\n      0.509627\n      0.099740\n      0.174744\n      0.106081\n      0.083801\n      0.026007\n      0.597762\n      0.134673\n      0.127823\n      0.057744\n      0.060519\n      0.021479\n      0.519414\n      0.140630\n      0.167105\n      0.080623\n      0.068262\n      0.023966\n      0.510118\n      0.185070\n      0.156140\n      0.068210\n      0.062267\n      0.018194\n      0.534198\n      0.161565\n      0.153183\n      0.065650\n      0.065872\n      0.019532\n      0.586799\n      0.139837\n      0.121766\n      0.056820\n      ...\n      0.068092\n      0.022911\n      0.209975\n      0.288701\n      0.212362\n      0.143128\n      0.140391\n      0.005442\n      0.211023\n      0.287946\n      0.212669\n      0.142524\n      0.140397\n      0.005442\n      0.236724\n      0.155031\n      0.223585\n      0.181925\n      0.178715\n      0.024020\n      0.189490\n      0.254528\n      0.207994\n      0.163306\n      0.176389\n      0.008293\n      0.720894\n      0.077628\n      0.080070\n      0.042218\n      0.057003\n      0.022188\n      0.761949\n      0.065250\n      0.071764\n      0.036249\n      0.047086\n      0.017702\n      Sleep stage W\n      PSG29.edf\n    \n    \n      3\n      0.496456\n      0.078696\n      0.145985\n      0.073315\n      0.168728\n      0.036820\n      0.415612\n      0.150171\n      0.162990\n      0.084801\n      0.145732\n      0.040695\n      0.610062\n      0.156481\n      0.096959\n      0.047532\n      0.071230\n      0.017735\n      0.481011\n      0.199789\n      0.142475\n      0.060787\n      0.085889\n      0.030048\n      0.532062\n      0.180797\n      0.130148\n      0.055573\n      0.080988\n      0.020431\n      0.493871\n      0.184382\n      0.134594\n      0.071134\n      0.086786\n      0.029233\n      0.543152\n      0.187464\n      0.110963\n      0.051094\n      ...\n      0.080532\n      0.022249\n      0.183674\n      0.338252\n      0.201751\n      0.136399\n      0.135297\n      0.004626\n      0.188792\n      0.336624\n      0.200293\n      0.135144\n      0.134579\n      0.004568\n      0.236594\n      0.153612\n      0.215825\n      0.169071\n      0.198682\n      0.026217\n      0.159044\n      0.288443\n      0.207075\n      0.160674\n      0.177189\n      0.007575\n      0.583039\n      0.117859\n      0.100131\n      0.045193\n      0.102050\n      0.051728\n      0.657698\n      0.107005\n      0.083062\n      0.039670\n      0.079945\n      0.032621\n      Sleep stage W\n      PSG29.edf\n    \n    \n      4\n      0.499096\n      0.090408\n      0.128249\n      0.119394\n      0.126275\n      0.036577\n      0.355009\n      0.114010\n      0.227973\n      0.142860\n      0.120449\n      0.039700\n      0.665468\n      0.097059\n      0.102477\n      0.055994\n      0.057387\n      0.021614\n      0.567872\n      0.108560\n      0.147445\n      0.074687\n      0.072923\n      0.028513\n      0.649271\n      0.120768\n      0.096039\n      0.061096\n      0.056664\n      0.016162\n      0.606666\n      0.124536\n      0.117359\n      0.064167\n      0.063759\n      0.023512\n      0.545407\n      0.135738\n      0.135981\n      0.053538\n      ...\n      0.102558\n      0.032455\n      0.191534\n      0.316908\n      0.221889\n      0.128541\n      0.135913\n      0.005214\n      0.188943\n      0.317868\n      0.222645\n      0.128482\n      0.136874\n      0.005188\n      0.316837\n      0.164763\n      0.211233\n      0.130939\n      0.150966\n      0.025262\n      0.172198\n      0.274478\n      0.218936\n      0.148539\n      0.178593\n      0.007258\n      0.718889\n      0.060736\n      0.083312\n      0.042121\n      0.064617\n      0.030325\n      0.728514\n      0.062010\n      0.082649\n      0.035992\n      0.059911\n      0.030924\n      Sleep stage W\n      PSG29.edf\n    \n  \n\n5 rows × 86 columns\n\n\n\nAdding to this, we are going to filter our data to keep only the complete patients. We can do so using the info.csv file we have previously created:\n\ndf_info = pd.read_csv(path_info)\ndf_info.head()\n\n\n\n\n\n  \n    \n      \n      File\n      DifferentStages\n      Complete\n    \n  \n  \n    \n      0\n      PSG29.edf\n      3\n      False\n    \n    \n      1\n      PSG12.edf\n      5\n      True\n    \n    \n      2\n      PSG17.edf\n      5\n      True\n    \n    \n      3\n      PSG10.edf\n      4\n      False\n    \n    \n      4\n      PSG23.edf\n      3\n      False\n    \n  \n\n\n\n\nIf we keep only the complete patients from this dataframe and join it with the other one, we can filter out the incomplete patients to train only with the complete ones:\n\ndf_info_complete = df_info[df_info.Complete]\nlen(df_info_complete)\n\n30\n\n\nWe see that we are left with 30 out of 36 patients. Next we’ll join both dataframes:\n\ndf_complete = df.merge(right=df_info_complete, how=\"right\", left_on=\"Patient\", right_on=\"File\")\ndf_complete = df_complete.drop([\"Patient\", \"DifferentStages\", \"Complete\"], axis=1)\ndf.shape, df_complete.shape\n\n((27680, 86), (24121, 86))\n\n\n\ndf_complete.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n      20\n      21\n      22\n      23\n      24\n      25\n      26\n      27\n      28\n      29\n      30\n      31\n      32\n      33\n      34\n      35\n      36\n      37\n      38\n      39\n      ...\n      46\n      47\n      48\n      49\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n      60\n      61\n      62\n      63\n      64\n      65\n      66\n      67\n      68\n      69\n      70\n      71\n      72\n      73\n      74\n      75\n      76\n      77\n      78\n      79\n      80\n      81\n      82\n      83\n      Label\n      File\n    \n  \n  \n    \n      0\n      0.828777\n      0.111004\n      0.036347\n      0.013696\n      0.008029\n      0.002147\n      0.866095\n      0.077208\n      0.029721\n      0.013948\n      0.010277\n      0.002751\n      0.893198\n      0.075901\n      0.015894\n      0.007203\n      0.006068\n      0.001736\n      0.897216\n      0.066448\n      0.016855\n      0.006646\n      0.010521\n      0.002313\n      0.839334\n      0.129764\n      0.020567\n      0.005832\n      0.003590\n      0.000913\n      0.874244\n      0.099975\n      0.015362\n      0.006066\n      0.003442\n      0.000911\n      0.871306\n      0.087882\n      0.021908\n      0.007706\n      ...\n      0.007495\n      0.002202\n      0.559114\n      0.187103\n      0.049373\n      0.076654\n      0.121630\n      0.006125\n      0.569851\n      0.184307\n      0.048731\n      0.073804\n      0.117428\n      0.005878\n      0.599962\n      0.072144\n      0.036558\n      0.069310\n      0.177553\n      0.044474\n      0.314669\n      0.139663\n      0.104543\n      0.142964\n      0.274585\n      0.023577\n      0.863405\n      0.085994\n      0.017926\n      0.008112\n      0.013523\n      0.011039\n      0.842687\n      0.082653\n      0.018246\n      0.008742\n      0.021826\n      0.025847\n      Sleep stage W\n      PSG12.edf\n    \n    \n      1\n      0.829209\n      0.104899\n      0.032051\n      0.022996\n      0.008892\n      0.001952\n      0.822251\n      0.105734\n      0.035186\n      0.024547\n      0.009898\n      0.002384\n      0.895753\n      0.062918\n      0.021768\n      0.011074\n      0.006787\n      0.001699\n      0.894197\n      0.061178\n      0.017440\n      0.012895\n      0.012034\n      0.002256\n      0.857363\n      0.103283\n      0.023190\n      0.010939\n      0.004160\n      0.001065\n      0.883609\n      0.079566\n      0.021131\n      0.010778\n      0.003813\n      0.001103\n      0.885997\n      0.071112\n      0.021702\n      0.012148\n      ...\n      0.007887\n      0.002063\n      0.545761\n      0.190174\n      0.052838\n      0.079403\n      0.126536\n      0.005288\n      0.550894\n      0.191843\n      0.051842\n      0.077397\n      0.122869\n      0.005155\n      0.589353\n      0.073115\n      0.034576\n      0.073617\n      0.185505\n      0.043835\n      0.287737\n      0.143439\n      0.113177\n      0.149858\n      0.284439\n      0.021351\n      0.897045\n      0.053111\n      0.016674\n      0.010652\n      0.013242\n      0.009276\n      0.873665\n      0.057653\n      0.016445\n      0.013007\n      0.019579\n      0.019652\n      Sleep stage W\n      PSG12.edf\n    \n    \n      2\n      0.842406\n      0.100744\n      0.029417\n      0.019372\n      0.006586\n      0.001473\n      0.870060\n      0.081720\n      0.022897\n      0.017082\n      0.006613\n      0.001629\n      0.909758\n      0.051352\n      0.021887\n      0.010397\n      0.005378\n      0.001229\n      0.921258\n      0.047191\n      0.015626\n      0.007201\n      0.007109\n      0.001616\n      0.895852\n      0.069141\n      0.022387\n      0.008700\n      0.003240\n      0.000680\n      0.905155\n      0.062947\n      0.019297\n      0.008643\n      0.003221\n      0.000736\n      0.904595\n      0.058342\n      0.020455\n      0.009654\n      ...\n      0.006376\n      0.001663\n      0.536013\n      0.199818\n      0.045428\n      0.075261\n      0.137987\n      0.005494\n      0.550753\n      0.195377\n      0.043902\n      0.072385\n      0.132292\n      0.005291\n      0.638595\n      0.061563\n      0.034590\n      0.063653\n      0.165439\n      0.036159\n      0.294978\n      0.148989\n      0.095591\n      0.137203\n      0.301252\n      0.021988\n      0.889069\n      0.060842\n      0.020352\n      0.010035\n      0.011098\n      0.008603\n      0.869873\n      0.061609\n      0.020109\n      0.010276\n      0.017389\n      0.020744\n      Sleep stage W\n      PSG12.edf\n    \n    \n      3\n      0.826268\n      0.114365\n      0.034281\n      0.016519\n      0.006972\n      0.001596\n      0.834174\n      0.086591\n      0.045124\n      0.020168\n      0.011114\n      0.002829\n      0.886998\n      0.068077\n      0.027164\n      0.009995\n      0.006277\n      0.001489\n      0.899655\n      0.052545\n      0.023429\n      0.009772\n      0.011983\n      0.002616\n      0.852234\n      0.110114\n      0.023666\n      0.009198\n      0.003865\n      0.000923\n      0.885270\n      0.079228\n      0.022851\n      0.008011\n      0.003732\n      0.000907\n      0.893624\n      0.060173\n      0.029191\n      0.008671\n      ...\n      0.007102\n      0.002081\n      0.558408\n      0.174786\n      0.052194\n      0.077200\n      0.131827\n      0.005585\n      0.563630\n      0.175305\n      0.051710\n      0.075393\n      0.128516\n      0.005446\n      0.645756\n      0.058029\n      0.035434\n      0.060650\n      0.158191\n      0.041940\n      0.283484\n      0.138802\n      0.114658\n      0.142425\n      0.298707\n      0.021923\n      0.887805\n      0.058118\n      0.023766\n      0.008757\n      0.012120\n      0.009433\n      0.873901\n      0.055302\n      0.022631\n      0.009176\n      0.018615\n      0.020375\n      Sleep stage W\n      PSG12.edf\n    \n    \n      4\n      0.892040\n      0.072569\n      0.016537\n      0.013553\n      0.004228\n      0.001073\n      0.887429\n      0.069672\n      0.017843\n      0.017650\n      0.005630\n      0.001776\n      0.949596\n      0.032758\n      0.009363\n      0.005098\n      0.002614\n      0.000571\n      0.924791\n      0.048967\n      0.010825\n      0.007579\n      0.006528\n      0.001310\n      0.928209\n      0.053080\n      0.011061\n      0.004976\n      0.002208\n      0.000466\n      0.927354\n      0.051554\n      0.012094\n      0.006003\n      0.002475\n      0.000520\n      0.937408\n      0.039077\n      0.012161\n      0.006905\n      ...\n      0.006053\n      0.001503\n      0.549693\n      0.203476\n      0.043815\n      0.072618\n      0.124463\n      0.005936\n      0.558714\n      0.200516\n      0.043050\n      0.071087\n      0.120879\n      0.005754\n      0.723112\n      0.048052\n      0.024039\n      0.054247\n      0.118286\n      0.032265\n      0.305310\n      0.156931\n      0.093174\n      0.134177\n      0.286451\n      0.023958\n      0.929078\n      0.041854\n      0.010671\n      0.006873\n      0.006212\n      0.005312\n      0.914073\n      0.043103\n      0.011320\n      0.007723\n      0.010389\n      0.013391\n      Sleep stage W\n      PSG12.edf\n    \n  \n\n5 rows × 86 columns\n\n\n\nAnd we see that we have removed the corresponding 3559 rows corresponding to incomplete patients."
  },
  {
    "objectID": "01_Experiments/basic_features.html#defining-the-metrics",
    "href": "01_Experiments/basic_features.html#defining-the-metrics",
    "title": "[REM/NO REM] Pre-extracted basic features",
    "section": "Defining the metrics",
    "text": "Defining the metrics\n\nAUC can be a good metrics when working with binary problems.\n\nTo measure the performance of our classifier, we’re going to use the AUC and the accuracy, the latter just for completion. Because we want to obtain the most realistic metric possible, we are going to perform a Patient-Fold where we train with all the patients and then test in a completelly different one. This will give us a good estimate of the generalization performance of our pipeline.\nThis can be achieved using cross_validate in conjunction with GroupKFold and setting the groups parameter to be the File column in our previous dataframe:\n\nmetrics = {\n    \"Accuracy\": \"accuracy\",\n    \"AUC\": \"roc_auc\",\n}\n\n\ncvg = cross_validate(RandomForestClassifier(), \n                     X=df_complete.drop([\"Label\", \"File\"], axis=1),\n                     y=df_complete[\"Label\"]==\"Sleep stage R\",\n                     scoring=metrics,\n                     return_train_score=True,\n                     cv=GroupKFold(n_splits=len(df_complete.File.unique())),\n                     groups=df_complete[\"File\"],\n                     n_jobs=5)\n\nCPU times: user 146 ms, sys: 104 ms, total: 250 ms\nWall time: 2min 37s\n\n\n\ncvg\n\n{'fit_time': array([27.03762555, 26.27106881, 26.99674702, 26.86529589, 26.24467683,\n        25.45121002, 23.67596412, 24.07731962, 24.42808008, 24.3764236 ,\n        25.62880945, 25.02343202, 24.51707554, 25.30792546, 25.24074316,\n        24.36477757, 24.58056784, 25.27363634, 24.7898047 , 24.37082696,\n        23.90679312, 24.51267362, 24.12866235, 24.50727344, 24.42071271,\n        24.26417112, 24.38420892, 24.73950148, 24.41905499, 24.89244962]),\n 'score_time': array([0.06685781, 0.06159592, 0.05747652, 0.05841327, 0.0619328 ,\n        0.05348492, 0.0628283 , 0.05884314, 0.06110263, 0.05982661,\n        0.05692792, 0.05831432, 0.05882573, 0.05572987, 0.06257844,\n        0.05658269, 0.05508018, 0.05524349, 0.0544374 , 0.05899739,\n        0.05471539, 0.05301929, 0.05601811, 0.05660391, 0.05370593,\n        0.05541277, 0.05205274, 0.05044127, 0.05035877, 0.04688859]),\n 'test_Accuracy': array([0.91080797, 0.94065934, 0.88548753, 0.904     , 0.86869871,\n        0.9144197 , 0.87573271, 0.90973036, 0.93184489, 0.81560284,\n        0.85      , 0.91746411, 0.87860577, 0.80853659, 0.81295844,\n        0.96924969, 0.75709001, 0.89440994, 0.98461538, 0.97293814,\n        0.87696335, 0.90944882, 0.94218134, 0.92368421, 0.91125828,\n        0.85066667, 0.95392954, 0.86573427, 0.83875969, 0.9492635 ]),\n 'train_Accuracy': array([1.        , 0.99995692, 1.        , 1.        , 0.99995702,\n        1.        , 1.        , 1.        , 1.        , 0.99995704,\n        0.99995705, 1.        , 1.        , 1.        , 0.99995709,\n        1.        , 1.        , 1.        , 1.        , 1.        ,\n        0.99995719, 1.        , 1.        , 1.        , 1.        ,\n        1.        , 1.        , 1.        , 1.        , 0.99995746]),\n 'test_AUC': array([0.50650465, 0.41852717, 0.80062372, 0.8343038 , 0.82448892,\n        0.75410263, 0.85782295, 0.94624178, 0.73788735, 0.77484245,\n        0.91584851, 0.24220774, 0.69733775, 0.40263327, 0.31369251,\n        0.45761421, 0.67280278, 0.83897209, 0.31743707, 0.32397487,\n        0.48285964, 0.66438714, 0.35597914, 0.68175239, 0.36966178,\n        0.8807308 , 0.30128259, 0.76383847, 0.65165771, 0.51384872]),\n 'train_AUC': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n\n\n\nplt.boxplot([cvg['test_AUC'], cvg['test_Accuracy']])\nplt.xticks(range(3), [\" \", \"AUC\", \"Accuracy\"])\nplt.title(\"Results of performing Patient-Fold with default RandomForest [REM/No REM]\")\nplt.show()"
  },
  {
    "objectID": "01_Experiments/basic_features_all_binary.html",
    "href": "01_Experiments/basic_features_all_binary.html",
    "title": "[All] Pre-extracted basic features",
    "section": "",
    "text": "In this quick experiment we are going to utilize the pre-extracted features to perform different binary classification problems: We’re going to try every possible binary problem to assess the difference in performance between them. Later we will jump into using an ensemble of binary classificators to predict all the classes."
  },
  {
    "objectID": "01_Experiments/basic_features_all_binary.html#load-and-filter-the-data",
    "href": "01_Experiments/basic_features_all_binary.html#load-and-filter-the-data",
    "title": "[All] Pre-extracted basic features",
    "section": "Load and filter the data",
    "text": "Load and filter the data\nThe first thing we have to do is loading the features we have previously extracted:\n\ndf = pd.read_csv(path_data, index_col=0)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      76\n      77\n      78\n      79\n      80\n      81\n      82\n      83\n      Label\n      Patient\n    \n  \n  \n    \n      0\n      0.495066\n      0.112756\n      0.112814\n      0.064382\n      0.165632\n      0.049350\n      0.362000\n      0.116360\n      0.171950\n      0.159524\n      ...\n      0.064126\n      0.025867\n      0.687305\n      0.084597\n      0.083275\n      0.047782\n      0.070681\n      0.026361\n      Sleep stage W\n      PSG29.edf\n    \n    \n      1\n      0.465074\n      0.117853\n      0.177654\n      0.101286\n      0.104765\n      0.033368\n      0.347451\n      0.205411\n      0.208832\n      0.093656\n      ...\n      0.050156\n      0.026275\n      0.811640\n      0.054457\n      0.050954\n      0.026253\n      0.037776\n      0.018920\n      Sleep stage W\n      PSG29.edf\n    \n    \n      2\n      0.493321\n      0.083727\n      0.160615\n      0.093394\n      0.127338\n      0.041605\n      0.509627\n      0.099740\n      0.174744\n      0.106081\n      ...\n      0.057003\n      0.022188\n      0.761949\n      0.065250\n      0.071764\n      0.036249\n      0.047086\n      0.017702\n      Sleep stage W\n      PSG29.edf\n    \n    \n      3\n      0.496456\n      0.078696\n      0.145985\n      0.073315\n      0.168728\n      0.036820\n      0.415612\n      0.150171\n      0.162990\n      0.084801\n      ...\n      0.102050\n      0.051728\n      0.657698\n      0.107005\n      0.083062\n      0.039670\n      0.079945\n      0.032621\n      Sleep stage W\n      PSG29.edf\n    \n    \n      4\n      0.499096\n      0.090408\n      0.128249\n      0.119394\n      0.126275\n      0.036577\n      0.355009\n      0.114010\n      0.227973\n      0.142860\n      ...\n      0.064617\n      0.030325\n      0.728514\n      0.062010\n      0.082649\n      0.035992\n      0.059911\n      0.030924\n      Sleep stage W\n      PSG29.edf\n    \n  \n\n5 rows × 86 columns\n\n\n\nAdding to this, we are going to filter our data to keep only the complete patients. We can do so using the info.csv file we have previously created:\n\ndf_info = pd.read_csv(path_info)\ndf_info.head()\n\n\n\n\n\n  \n    \n      \n      File\n      DifferentStages\n      Complete\n    \n  \n  \n    \n      0\n      PSG29.edf\n      3\n      False\n    \n    \n      1\n      PSG12.edf\n      5\n      True\n    \n    \n      2\n      PSG17.edf\n      5\n      True\n    \n    \n      3\n      PSG10.edf\n      4\n      False\n    \n    \n      4\n      PSG23.edf\n      3\n      False\n    \n  \n\n\n\n\nIf we keep only the complete patients from this dataframe and join it with the other one, we can filter out the incomplete patients to train only with the complete ones:\n\ndf_info_complete = df_info[df_info.Complete]\nlen(df_info_complete)\n\n30\n\n\nWe see that we are left with 30 out of 36 patients. Next we’ll join both dataframes:\n\ndf_complete = df.merge(right=df_info_complete, how=\"right\", left_on=\"Patient\", right_on=\"File\")\ndf_complete = df_complete.drop([\"Patient\", \"DifferentStages\", \"Complete\"], axis=1)\ndf.shape, df_complete.shape\n\n((27680, 86), (24121, 86))\n\n\n\ndf_complete.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      76\n      77\n      78\n      79\n      80\n      81\n      82\n      83\n      Label\n      File\n    \n  \n  \n    \n      0\n      0.828777\n      0.111004\n      0.036347\n      0.013696\n      0.008029\n      0.002147\n      0.866095\n      0.077208\n      0.029721\n      0.013948\n      ...\n      0.013523\n      0.011039\n      0.842687\n      0.082653\n      0.018246\n      0.008742\n      0.021826\n      0.025847\n      Sleep stage W\n      PSG12.edf\n    \n    \n      1\n      0.829209\n      0.104899\n      0.032051\n      0.022996\n      0.008892\n      0.001952\n      0.822251\n      0.105734\n      0.035186\n      0.024547\n      ...\n      0.013242\n      0.009276\n      0.873665\n      0.057653\n      0.016445\n      0.013007\n      0.019579\n      0.019652\n      Sleep stage W\n      PSG12.edf\n    \n    \n      2\n      0.842406\n      0.100744\n      0.029417\n      0.019372\n      0.006586\n      0.001473\n      0.870060\n      0.081720\n      0.022897\n      0.017082\n      ...\n      0.011098\n      0.008603\n      0.869873\n      0.061609\n      0.020109\n      0.010276\n      0.017389\n      0.020744\n      Sleep stage W\n      PSG12.edf\n    \n    \n      3\n      0.826268\n      0.114365\n      0.034281\n      0.016519\n      0.006972\n      0.001596\n      0.834174\n      0.086591\n      0.045124\n      0.020168\n      ...\n      0.012120\n      0.009433\n      0.873901\n      0.055302\n      0.022631\n      0.009176\n      0.018615\n      0.020375\n      Sleep stage W\n      PSG12.edf\n    \n    \n      4\n      0.892040\n      0.072569\n      0.016537\n      0.013553\n      0.004228\n      0.001073\n      0.887429\n      0.069672\n      0.017843\n      0.017650\n      ...\n      0.006212\n      0.005312\n      0.914073\n      0.043103\n      0.011320\n      0.007723\n      0.010389\n      0.013391\n      Sleep stage W\n      PSG12.edf\n    \n  \n\n5 rows × 86 columns\n\n\n\nAnd we see that we have removed the corresponding 3559 rows corresponding to incomplete patients."
  },
  {
    "objectID": "01_Experiments/basic_features_all_binary.html#defining-the-metrics",
    "href": "01_Experiments/basic_features_all_binary.html#defining-the-metrics",
    "title": "[All] Pre-extracted basic features",
    "section": "Defining the metrics",
    "text": "Defining the metrics\n\nAUC can be a good metrics when working with binary problems.\n\nTo measure the performance of our classifier, we’re going to use the AUC and the accuracy, the latter just for completion. Because we want to obtain the most realistic metric possible, we are going to perform a Patient-Fold where we train with all the patients and then test in a completelly different one. This will give us a good estimate of the generalization performance of our pipeline.\n\nThis can be achieved using cross_validate in conjunction with GroupKFold and setting the groups parameter to be the File column in our previous dataframe.\n\nWe will perform a loop to perform cross-validation on each of the five possible binary classification problems:\n\nmetrics_score = {\n    \"Accuracy\": \"accuracy\",\n    \"AUC\": \"roc_auc\",\n}\n\n\npos_labels = df_complete.Label.unique()\npos_labels\n\narray(['Sleep stage W', 'Sleep stage N1', 'Sleep stage N2',\n       'Sleep stage N3', 'Sleep stage R'], dtype=object)\n\n\n\nresults = {}\nfor label in track(pos_labels):\n    cvg = cross_validate(RandomForestClassifier(), \n                         X=df_complete.drop([\"Label\", \"File\"], axis=1),\n                         y=df_complete[\"Label\"]==label,\n                         scoring=metrics_score,\n                         return_train_score=True,\n                         cv=GroupKFold(n_splits=len(df_complete.File.unique())),\n                         groups=df_complete[\"File\"],\n                         n_jobs=6)\n    results[label] = cvg\n\n\n\n\n\n\n\n\n\n\nFor plotting purpouses we can build a huge dataframe with all the data we obtained:\n\ndfs = []\n\nfor label, res in results.items():\n    df_res_i = pd.DataFrame.from_dict(res)\n    df_res_i['Binary'] = label\n    dfs.append(df_res_i)\n\n\ndfs_total = pd.concat(dfs)\nprint(dfs_total.shape)\ndfs_total.head()\n\n(150, 7)\n\n\n\n\n\n\n  \n    \n      \n      fit_time\n      score_time\n      test_Accuracy\n      train_Accuracy\n      test_AUC\n      train_AUC\n      Binary\n    \n  \n  \n    \n      0\n      38.173372\n      0.074652\n      0.732424\n      1.0\n      0.907228\n      1.0\n      Sleep stage W\n    \n    \n      1\n      36.354859\n      0.068716\n      0.663736\n      1.0\n      0.752617\n      1.0\n      Sleep stage W\n    \n    \n      2\n      35.902291\n      0.092104\n      0.828798\n      1.0\n      0.891056\n      1.0\n      Sleep stage W\n    \n    \n      3\n      36.459545\n      0.093932\n      0.952000\n      1.0\n      0.968489\n      1.0\n      Sleep stage W\n    \n    \n      4\n      35.683652\n      0.098985\n      0.866354\n      1.0\n      0.915691\n      1.0\n      Sleep stage W\n    \n  \n\n\n\n\n\ndfs_total.drop([\"fit_time\", \"score_time\", \"train_Accuracy\", \"train_AUC\"], axis=1)\\\n         .melt(id_vars='Binary')\\\n         .pipe((sns.boxplot, \"data\"), x=\"variable\", y=\"value\", hue=\"Binary\")\nplt.xlabel(\"Metric name\")\nplt.ylabel(\"Metric value\")\nplt.title(\"Patient-Fold test results for different binary problems\")\nplt.show()\n\n\n\n\n\ndfs_total.drop([\"fit_time\", \"score_time\", \"test_Accuracy\", \"test_AUC\"], axis=1)\\\n         .melt(id_vars='Binary')\\\n         .pipe((sns.boxplot, \"data\"), x=\"variable\", y=\"value\", hue=\"Binary\")\nplt.xlabel(\"Metric name\")\nplt.ylabel(\"Metric value\")\nplt.title(\"Patient-Fold train results for different binary problems\")\nplt.show()"
  },
  {
    "objectID": "01_Experiments/basic_features_own_patient.html",
    "href": "01_Experiments/basic_features_own_patient.html",
    "title": "Using basic features within the same patient",
    "section": "",
    "text": "import os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict\n\nfrom rich.progress import track\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *\nfrom sleepstagingidal.feature_extraction import *"
  },
  {
    "objectID": "01_Experiments/basic_features_own_patient.html#looping-through-patients",
    "href": "01_Experiments/basic_features_own_patient.html#looping-through-patients",
    "title": "Using basic features within the same patient",
    "section": "Looping through patients",
    "text": "Looping through patients\n\nAs we only want to perform a very basic check, we are going to be looping through all the patients.\n\n\nresults = {}\n\nfor path in track(path_files):\n    file_name = path.split(\"/\")[-1]\n    raw = read_clean_edf(path, resample=100, bandpass=(0.3, 49))\n    epochs, sr = get_epochs(raw, channels=channels)\n    bandpowers = calculate_bandpower(epochs, sf=sr)\n    labels = epochs.events[:,-1]\n    results_cv = cross_validate(RandomForestClassifier(random_state=42), bandpowers, labels)\n    results[file_name] = results_cv['test_score']\n\nWe can put the logged results into a DataFrame and save them as .csv to avoid having to repeat the experiment:\n\ndf = pd.DataFrame.from_dict(results, orient='index')\ndf.index.set_names(\"File\", inplace=True)\ndf['Mean'] = df.mean(axis=1)\ndf['Std'] = df.std(axis=1)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      Mean\n      Std\n    \n    \n      File\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      PSG29.edf\n      0.548611\n      0.472222\n      0.444444\n      0.321678\n      0.517483\n      0.460888\n      0.078328\n    \n    \n      PSG12.edf\n      0.411765\n      0.392157\n      0.614379\n      0.398693\n      0.473684\n      0.458136\n      0.083295\n    \n    \n      PSG17.edf\n      0.349693\n      0.561728\n      0.537037\n      0.506173\n      0.209877\n      0.432902\n      0.133771\n    \n    \n      PSG10.edf\n      0.657895\n      0.801325\n      0.629139\n      0.344371\n      0.576159\n      0.601778\n      0.148749\n    \n    \n      PSG23.edf\n      0.873418\n      0.815287\n      0.821656\n      0.878981\n      0.605096\n      0.798887\n      0.100312\n    \n  \n\n\n\n\n\ndf.to_csv(\"Results/00_basic_features_own_patient.csv\")"
  },
  {
    "objectID": "feature_extraction.html",
    "href": "feature_extraction.html",
    "title": "Feature extraction",
    "section": "",
    "text": "from multiprocessing.spawn import import_main_path\nimport os\nfrom glob import glob\nfrom collections import Counter\nfrom typing import List, Dict, Tuple\n\nfrom rich.progress import track\nimport numpy as np\nimport pandas as pd\nimport mne\nimport yasa\n\nfrom sleepstagingidal.data import *\nfrom sleepstagingidal.dataa import *\nfrom sleepstagingidal.dataa import swap_dict"
  },
  {
    "objectID": "feature_extraction.html#amplitude-independent-features",
    "href": "feature_extraction.html#amplitude-independent-features",
    "title": "Feature extraction",
    "section": "Amplitude-independent features",
    "text": "Amplitude-independent features\nIn our case, we have downsampled our data and have 3000 data points per epoch (30s fragment of the recording), but this is a huge amount of features to be fed into a model. Getting even further, we have the problem of calibration and normalization, which is aggravated even more when working with medical data (each hospital may work slightly different). Because of this, we are going to employ features that are independent of the amplitude of the signal: we are going to utilize mainly frequency-related features.\nWe can use the library yasa to perform a basic feature extraction:\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\nBefore extracting the features, we are going to perform downsampling and a bandpass filter to keep only the low frequencies:\n\nsource\n\nread_clean_edf\n\n read_clean_edf (path, resample:int=None,\n                 bandpass:Tuple[float,float]=None, verbose:bool=False)\n\nLoads and (potentially) cleans an .edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\n\n\nPath to an .edf file.\n\n\nresample\nint\nNone\nFrequency (Hz) we want to resample to. If None, no resampling is applied.\n\n\nbandpass\nTuple\nNone\nTuple (min_freq, max_freq) for the bandpass filter. If None, no bandpass is applied.\n\n\nverbose\nbool\nFalse\nQuantity of information to be shown.\n\n\nReturns\nRawEDF\n\nRaw object with the corresponding cleaning,\n\n\n\n\nraw = read_clean_edf(path_files[0], resample=100, bandpass=(0.3, 49))\nraw\n\nOnce it’s done, we can use the function yasa.bandpower() to extract frequency-related features from the data. Keep in mind that this function expects the data to be fed in epochs form, so we have to transform it first:\n\nepochs, sr = get_epochs(raw, channels=channels, verbose=False)\nepochs\n\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\n\n\n    \n        Number of events\n        718\n    \n    \n        Events\n        \n        Sleep stage N1: 29Sleep stage N2: 323Sleep stage W: 366\n        \n    \n    \n        Time range\n        0.000 – 29.990 sec\n    \n    \n        Baseline\n        off\n    \n\n\n\n\nsource\n\n\ncalculate_bandpower\n\n calculate_bandpower (epochs, sf=100)\n\nExtracts the bandpower per epoch and returns it as an array ready to be fed into a model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\n\n\nEpochs object or 3D array [Epochs, Channels, Data].\n\n\nsf\nint\n100\nSampling frequency of the data.\n\n\nReturns\narray\n\nNumpy array of shape [Epochs, 6*Channels] representing 6 bands.\n\n\n\n\nbandpowers = calculate_bandpower(epochs, sf=sr)\n\nCPU times: user 4.34 s, sys: 0 ns, total: 4.34 s\nWall time: 4.34 s\n\n\nIf we check the shape of the produced array, we find that we have quite reduced the dimensionality of the problem but, are we still able to classify the different sleep stages with this features?\n\nbandpowers.shape\n\n(718, 84)"
  },
  {
    "objectID": "feature_extraction.html#simple-model",
    "href": "feature_extraction.html#simple-model",
    "title": "Feature extraction",
    "section": "Simple model",
    "text": "Simple model\n\nTo check the usability of this features we will train a very simple model performing a random split within the same recording. As we are building our project sequentially, we want to make sure that the things we do are usable as building blocks for the next things.\n\nFirst of all, we can extract the labels from the epochs object we have already created:\n\nlabels = epochs.events[:,-1]\nlabels.shape\n\n(718,)\n\n\nNext, we can randomly split our data and train a simple default random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(bandpowers, labels, test_size=0.2, random_state=42)\n\nassert X_train.shape[0] == Y_train.shape[0]\nassert X_test.shape[0] == Y_test.shape[0]\nassert X_train.shape[1] == X_test.shape[1]\n\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, Y_train)\n\nCPU times: user 423 ms, sys: 3.37 ms, total: 426 ms\nWall time: 444 ms\n\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nmodel.score(X_train, Y_train), model.score(X_test, Y_test)\n\n(1.0, 0.5972222222222222)\n\n\nHaving performed this simple check, we know that the features extracted hold, at least, some useful information that can be used to predict the different sleep stages. We can continue adding complexity to our pipeline."
  },
  {
    "objectID": "data_loading.html",
    "href": "data_loading.html",
    "title": "Data loading",
    "section": "",
    "text": "Stablishing a data-loading pipeline is always a good starting point for starting every project. In this case in particular, we have to be conscious because each .edf file we are going to work with is about 1 Gb, so performing lazy loading might become crucial for the project."
  },
  {
    "objectID": "data_loading.html#loading-.edf-files",
    "href": "data_loading.html#loading-.edf-files",
    "title": "Data loading",
    "section": "Loading .edf files",
    "text": "Loading .edf files\n\nWe will be using mne.io.read_raw_edf.\n\n\npath_files = glob(os.path.join(path_data, \"*.edf\"))\n\n\nraw = mne.io.read_raw_edf(path_files[0], preload=False)\nraw\n\nExtracting EDF parameters from /media/2tbraid/antonia/PSG/PSG29.edf...\nEDF file detected\nSetting channel info structure...\nCreating raw.info structure...\n\n\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Omitted 200 annotation(s) that were outside data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n/tmp/ipykernel_227803/1189779846.py:1: RuntimeWarning: Limited 2 annotation(s) that were expanding outside the data range.\n  raw = mne.io.read_raw_edf(path_files[0], preload=False)\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        512.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.00 Hz\n    \n    \n    \n    \n        Lowpass\n        256.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nNow that we’ve loaded the file, we can downsample it to 100Hz and apply a low-pass filter to the signal:\n\n# Downsample the data to 100 Hz\nraw.resample(100)\n# Apply a bandpass filter from 0.3 to 49 Hz\nraw.filter(0.3, 49)\n\nFiltering raw data in 1 contiguous segment\nSetting up band-pass filter from 0.3 - 49 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 0.30\n- Lower transition bandwidth: 0.30 Hz (-6 dB cutoff frequency: 0.15 Hz)\n- Upper passband edge: 49.00 Hz\n- Upper transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 49.50 Hz)\n- Filter length: 1101 samples (11.010 sec)\n\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.5s finished\n\n\n\n\n    \n        Measurement date\n        \n        January 01, 2019  22:48:22 GMT\n        \n    \n    \n        Experimenter\n        \n        Unknown\n        \n    \n        Participant\n        \n        Unknown\n        \n    \n    \n        Digitized points\n        \n        Not available\n        \n    \n    \n        Good channels\n        50 EEG\n    \n    \n        Bad channels\n        None\n    \n    \n        EOG channels\n        Not available\n    \n    \n        ECG channels\n        Not available\n    \n    \n        Sampling frequency\n        100.00 Hz\n    \n    \n    \n    \n        Highpass\n        0.30 Hz\n    \n    \n    \n    \n        Lowpass\n        49.00 Hz\n    \n    \n    \n    \n    \n        Filenames\n        PSG29.edf\n    \n    \n    \n        Duration\n        06:37:04 (HH:MM:SS)\n    \n\n\n\nThe different channels of the signal are available in the attribute .ch_names:\n\nprint(len(raw.ch_names))\nraw.ch_names\n\n50\n\n\n['C3',\n 'C4',\n 'O1',\n 'O2',\n 'A1',\n 'A2',\n 'Cz',\n 'F3',\n 'F4',\n 'F7',\n 'F8',\n 'Fz',\n 'Fp1',\n 'Fp2',\n 'Fpz',\n 'P3',\n 'P4',\n 'Pz',\n 'T3',\n 'T4',\n 'T5',\n 'T6',\n 'LOC',\n 'ROC',\n 'CHIN1',\n 'CHIN2',\n 'ECGL',\n 'ECGR',\n 'LAT1',\n 'LAT2',\n 'RAT1',\n 'RAT2',\n 'CHEST',\n 'ABD',\n 'FLOW',\n 'SNORE',\n 'DIF5',\n 'DIF6',\n 'POS',\n 'DC2',\n 'DC3',\n 'DC4',\n 'DC5',\n 'DC6',\n 'DC7',\n 'DC8',\n 'DC9',\n 'DC10',\n 'OSAT',\n 'PR']\n\n\nWe can extract a subset of the channels by using the methods .pick_channels() and .drop_channels():\n\n# Select a subset of EEG channels\n# raw.pick_channels(['LOC-A2', 'ROC-A1', 'F3-A2', 'C3-A2', 'O1-A2', 'F4-A1', 'C4-A1', 'O2-A1', 'X1', 'X2', 'X3'])"
  },
  {
    "objectID": "data_loading.html#making-use-of-the-annotations-in-the-recordings",
    "href": "data_loading.html#making-use-of-the-annotations-in-the-recordings",
    "title": "Data loading",
    "section": "Making use of the annotations in the recordings",
    "text": "Making use of the annotations in the recordings\nOne of the peculiarities of this kind of data is they include medical annotations. We can access them in the atribute .annotations:\n\nraw.annotations\n\n<Annotations | 2057 segments: Central Apnea (6), EEG arousal (131), ...>\n\n\nWe can index this object to view more information:\n\nraw.annotations[80]\n\nOrderedDict([('onset', 3420.0),\n             ('duration', 30.0),\n             ('description', 'Sleep stage W'),\n             ('orig_time',\n              datetime.datetime(2019, 1, 1, 22, 48, 22, tzinfo=datetime.timezone.utc))])\n\n\nAs we can see, they include all the information needed to split the signal into epochs:\n\nonset: starting time of an epoch.\nduration: duration of the epoch.\ndescription: label set by the medical staff.\norig_time: date when the data was collected.\n\nWe can use a Counter to count the different labels available in the data:\n\ncntr = Counter(raw.annotations.description)\ncntr\n\nCounter({'Montage:PR, Ref': 2,\n         'Start Recording': 1,\n         'Recording Analyzer - Sleep Events': 1,\n         'Recording Analyzer - Auto-Staging': 1,\n         'Recording Analyzer - ECG': 1,\n         'Recording Analyzer - Data Trends': 1,\n         'Video Recording ON': 1,\n         'Impedance at 10 kOhm': 1,\n         'Obstructive Apnea': 200,\n         'Patient Event': 2,\n         'Sleep stage W': 367,\n         'Lights Off': 1,\n         'Started Analyzer - Sleep Events': 1,\n         'Gain/Filter Change': 2,\n         'Oxygen Desaturation': 341,\n         'Oximeter Event': 272,\n         'Limb Movement': 109,\n         'Sleep stage N1': 29,\n         'EEG arousal': 131,\n         'Snoring': 251,\n         'Sleep stage N2': 323,\n         'Talking': 8,\n         'ORINANDO': 2,\n         'Hypopnea': 3,\n         'Central Apnea': 6})\n\n\n\nplt.figure()\nplt.bar(cntr.keys(), cntr.values())\nplt.xticks(rotation=90)\nplt.xlabel(\"Annotation\")\nplt.ylabel(\"Counts\")\nplt.title(\"Count of each annotation\")\nplt.show()\n\n\n\n\nAs we can see, there are a lot of epochs marked with labels we don’t care about. Later on we will see how we can filter the epochs to keep only the ones we care about."
  },
  {
    "objectID": "data_loading.html#extracting-epochs-from-the-full-recordings",
    "href": "data_loading.html#extracting-epochs-from-the-full-recordings",
    "title": "Data loading",
    "section": "Extracting epochs from the full recordings",
    "text": "Extracting epochs from the full recordings\nTo be able to use the data, we need to extract 30s epochs from the full recordings. We can do that easily with the function mne.make_fixed_length_epochs(), but we can make use of the included annotations and use mne.events_from_annotations() and mne.Epochs to directly obtain the epochs paired with their associated label.\n\nmne.events_from_annotations() allows us to filter by regexp, so we will be able to keep only the epochs corresponding to the labels we care about.\n\n\nsource\n\nget_epochs\n\n get_epochs (data:mne.io.edf.edf.RawEDF, channels:List[str]=None,\n             verbose:bool=False)\n\nExtracts labelled epochs from an already loaded raw edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nRawEDF\n\nRaw file loaded with mne.io.read_raw_edf.\n\n\nchannels\nList\nNone\nList of channels to keep. If None all the channels are kept.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\nTuple\n\nEpochs object and samplign rate of the signal.\n\n\n\n\nsource\n\n\nget_epochs_from_path\n\n get_epochs_from_path (path:str, channels:List[str]=None,\n                       verbose:bool=False)\n\nExtracts labelled epochs from an .edf file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nPath to an .edf file.\n\n\nchannels\nList\nNone\nList of channels to keep. If None all the channels are kept.\n\n\nverbose\nbool\nFalse\nAmount of information shown when loading the file.\n\n\nReturns\nTuple\n\nEpochs object and samplign rate of the signal.\n\n\n\n\nepochs, sr = get_epochs(raw)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nWhen this objects are created without preloading=True, we need to use the method .get_data() to access the data. As this kind of data uses a lot of memmory, the data of each epoch only are loaded when this method is called. We can check the shape of an epoch:\n\nepochs[0].get_data().shape\n\nUsing data from preloaded Raw for 1 events and 3000 original time points ...\n\n\n(1, 50, 3000)\n\n\nThe dimmensions of an epoch are: (1, Number of channels, Data points per channel).\n\nActually, the first dimmension corresponds to the number of epochs loaded, but we only loaded one epoch.\n\nLastly, this object stores two important atributes:\n\n.events: contains the parsed labels in the last dimension.\n.event_id: dictionary mapping the parsed labels to the original labels.\n\n\nepochs.events\n\narray([[ 228000,       0,       3],\n       [ 231000,       0,       3],\n       [ 234000,       0,       3],\n       ...,\n       [2373000,       0,       3],\n       [2376000,       0,       3],\n       [2379000,       0,       3]])\n\n\n\nepochs.event_id\n\n{'Sleep stage N1': 1, 'Sleep stage N2': 2, 'Sleep stage W': 3}\n\n\nComes to our attention that this patient has only been annotated with 3 out of 5 of the possible sleep stages. This might be important to look at to differentiate from full labeled patients and the rest."
  },
  {
    "objectID": "data_loading.html#inter-channel-operations",
    "href": "data_loading.html#inter-channel-operations",
    "title": "Data loading",
    "section": "Inter-channel operations",
    "text": "Inter-channel operations\nReal practitioners don’t actually use the data as we have imported it. They only utilize 9 channels, and perform reference operations between them. We can now reload the data using only this channels to save memory and perform the reference operations:\n\nchannels = [\"C3\", \"C4\", \"A1\", \"A2\", \"O1\", \"O2\", \"LOC\", \"ROC\", \"LAT1\", \"LAT2\", \"ECGL\", \"ECGR\", \"CHIN1\", \"CHIN2\"]\n\n\nepochs, sr = get_epochs(raw, channels=channels)\n\nUsed Annotations descriptions: ['Sleep stage N1', 'Sleep stage N2', 'Sleep stage W']\nNot setting metadata\n719 matching events found\nNo baseline correction applied\n0 projection items activated\nUsing data from preloaded Raw for 719 events and 3000 original time points ...\n1 bad epochs dropped\n\n\nThe reference operations they use are:\n\\[ C3 - \\frac{A1+A2}{2} \\] \\[ C4 - \\frac{A1+A2}{2} \\] \\[ 01 - \\frac{A1+A2}{2} \\] \\[ 02 - \\frac{A1+A2}{2} \\] \\[ LOC - A2 \\] \\[ ROC - A1 \\] \\[ LAT1 - LAT2 \\] \\[ ECGL - ECGR \\] \\[ CHIN1 - CHIN2 \\]\nIn order to mantain the code clean, we can put our data into a dictionary and then perform the operations. But fist we will load all the data in memory:\n\nX = epochs.get_data()\nY = epochs.events[:,-1]\nX.shape, Y.shape\n\nUsing data from preloaded Raw for 718 events and 3000 original time points ...\n\n\n((718, 14, 3000), (718,))\n\n\n\n# We need to expand_dims to keep the channels dim\ndata = {ch:np.expand_dims(X[:,i,:],1) for i, ch in enumerate(channels)}\n\n\nchannel1 = data[\"C3\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel2 = data[\"C4\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel3 = data[\"O1\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel4 = data[\"O2\"] - (data[\"A1\"]+data[\"A2\"])/2\nchannel5 = data[\"LOC\"] - data[\"A2\"]\nchannel6 = data[\"ROC\"] - data[\"A1\"]\nchannel7 = data[\"LAT1\"] - data[\"LAT2\"]\nchannel8 = data[\"ECGL\"] - data[\"ECGR\"]\nchannel9 = data[\"CHIN1\"] - data[\"CHIN2\"]\n\nNow we can concatenate the new channels into a new array:\n\nX = np.concatenate([channel1, channel2, channel3, channel4, channel5, channel6, channel7, channel8, channel9],1)\nX.shape\n\n(718, 9, 3000)"
  }
]